%\clearpage
A {\em predicate} is a Boolean combination of $Linconstr$.
Let $P$ and $Q$ be a predicate over input and output layers respectively.
A verification query is a triple $\langle N, P, Q \rangle$.
We need to prove that for each input $\boldsymbol{v}$,
if $\boldsymbol{v} \models P$, $N(\boldsymbol{v}) \models Q$.
We assume $P$ is of the form
% Let us say $P$ is a predicate on input layer's variables
$\Land_{i=1}^{|l_0|}lb_i \leq x_{0i} \leq ub_i$, where $lb_i$ and $ub_i$ are lower and upper bounds respectively for a neuron $n_{0i}$.
An input vector $\boldsymbol{v} \in \reals^{|l_0|}$ is a counter-example(cex) if $N(\boldsymbol{v}) \models \lnot Q$. 
Let us say $satval_{ij}$ represents the satisfying value of variable $x_{ij}$, after an optimization query. 

% The goal is to find an input $\boldsymbol{x}$, such that the predicates $P(\boldsymbol{x})$ and 
% $Q(N(\boldsymbol{x}))$ holds. The predicate $Q$ usually is the negation of the desired property. 
% The triple  is our verification query.
%  \todo{How to formalize $P$ and $Q$}




% We will use the following convention in writing our algorithms.


% \begin{itemize}
% \item 
% \item 
% % \item $W_i, B_i$ represent the weight and bias matrix of layer $l_i$ respectively. 
% \end{itemize}


% \begin{df}
% \end{df}

Let us say $lc_i = \Land_{j=1}^{|l_i|} A(n_{ij}).lexpr \leq x_{ij} \leq  A(n_{ij}).uexpr$ is a 
conjunction of upper and lower constraints of each neuron of layer $l_i$ with respect to abstract constraint $A$.
The $lexpr$ and $uexpr$ for any neuron of a layer contain variables only from the previous layer's neurons, 
hence $lc_i$ contains the variables from layers $l_{i-1}$ and $l_i$. 

\begin{df}
  A sequence of value vectors $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$ is an 
  {\em abstract execution} of abstract constraint $A$ if 
  $\boldsymbol{v_0} \models lc_0$ and $\boldsymbol{v_{i-1}}, \boldsymbol{v_i} \models lc_i$ for each $i \in [1,k]$.  
 Moreover, an abstract execution $\boldsymbol{v_0,...,v_k}$ is
 an {\em abstract counter example(abstractCEX)} if $\boldsymbol{v_k} \models \lnot Q$.
\end{df}



\texttt{DeepPoly} is a sound and incomplete technique because it does over-approximation analysis. 
If \deeppoly{} verifies the property then the property is guaranteed to be verified, otherwise, the result of it is unknown. 
We overcome this limitation by using a cegar-based technique, which is a complete technique and rely on \deeppoly{}. 
The algorithm~\ref{algo:main} represents the high-level flow of our approach.
The first line of the algorithm generates all the abstract constraints by using \deeppoly{}. 
These abstract constraints are the lower and upper constraints as well as the lower and upper bounds 
of each neuron in the neural network. Algorithms \ref{algo:verif1} and \ref{algo:verif2} verify the query 
and return either the verification successful or abstractCEX. 

If these algorithms return verification successful then we report verified,
otherwise analyze the abstractCEX by algorithm \ref{algo:refine2} and return the causes of spuriousness (markedNeurons). 
Algorithms \ref{algo:verif1} and \ref{algo:verif2} exploit the markedNeurons to verify the property. 

Our approach contains two parts, the first part contains an approach to finding the markedNeurons. 
The second part contains the verification approaches (utilizing markedNeurons).

\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State $A := deeppoly(N,P,Q)$\Comment{deeppoly generate the abstract constraints}
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or abstractCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{abstractCEX is a cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} cex
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\subsection{Causes of spuriousness} 
We have a following approach to find the causes of spuriousness (markedNeurons). 

\subsubsection{Optimization based approach: }

Let us say the abstractCEX is $\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$. 
The algorithm return $\boldsymbol{v_0}$ as a counter-example if $\boldsymbol{val_k^{\boldsymbol{v_0}}}$ 
does not satisfy the property $Q$. 
Since $l_0$ is an input layer, so, $\boldsymbol{v_0}$ and $\boldsymbol{val_0^{v_0}}$ are equal, 
and by theorem \ref{th:marked2} $\boldsymbol{v_1}$ and $\boldsymbol{val_1^{v_1}}$ are also equal. 
The core idea of this algorithm is to make $\boldsymbol{v'_2}$ as close as possible to $\boldsymbol{val_2^{v_0}}$, 
such that $\boldsymbol{v_0}, \boldsymbol{v_1}, \boldsymbol{v'_2}, ... \boldsymbol{v_k}$ become an abstractCEX. 
Here the closeness measured by the number of elements of $\boldsymbol{v'_2}$ becomes equal to the 
corresponding element of vector $\boldsymbol{val_2^{v_0}}$.
\begin{itemize}
  \item If $\boldsymbol{v'_2}$ becomes equal to $\boldsymbol{val_2^{v_0}}$ then $\boldsymbol{v'_3}$ will also become 
    equal to $\boldsymbol{val_3^{v_0}}$ by theorem \ref{th:marked2}. 
    Now we move on to the next relu layer $l_4$ and try to find the similar point $\boldsymbol{v'_4}$, such that 
    $\boldsymbol{v_0}, \boldsymbol{v_1}, \boldsymbol{v_2}, \boldsymbol{v'_4}... \boldsymbol{v_k}$ becomes an abstractCEX. 
    We repeat this process until the following case occurs. 
  \item If $\boldsymbol{v'_2}$ does not equal to $\boldsymbol{val_2^{v_0}}$ then we collect the neurons whose values differ, 
        and return them as a set of markedNeurons. 
\end{itemize}



\begin{theorem}
  \label{th:marked2}
  Let us say $\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$ is an abstract execution. 
  For an affine layer $l_i$, $\boldsymbol{val_i^{\boldsymbol{v_0}}} = \boldsymbol{v_i}$ if $\boldsymbol{val_{i-1}^{\boldsymbol{v_0}}} = \boldsymbol{v_{i-1}}$.  
  Since affine layer's constraints are exact.  
\end{theorem}
 

\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons1 \\
  \textbf{Input: } $\langle N,P,Q \rangle,A,markedNeurons,abstractCEX = \boldsymbol{v_0}, \boldsymbol{v_1} ... \boldsymbol{v_k}$\\
  \textbf{Output: } markedNeurons or real cex. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models \neg Q$. 
    \State Let us say $\boldsymbol{val_{i}}$ is the value vector at layer $l_i$, when $\boldsymbol{v_0}$ executed on network $N$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      \If{$l_i$ is \relu{} layer}
        \State $constr := \Land_{t=i}^k lc_t$
        \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref{eq:reluexact}}
        \State $constr := constr \land \Land_{j=1}^{|l_{i-1}|} (x_{(i-1)j} = val_{i-1}(j))$
        \State $constr := constr \land \Land_{j=1}^{|l_k|} (x_{kj} = v_k(j))$
        \State $softConstrs := \cup_{j=1}^{|l_j|} (x_{ij} = val_i(j))$
        \State $res, softsatSet = maxsoftConstr(constr, softConstrs)$ \Comment{res always \texttt{SAT}}
        \State $markedNt := \{n_{ij} | 1 \leq j \leq |l_i| \land (x_{ij} = val_i(j)) \notin  softsatSet\}$ 
        \If{$markedNt$ is empty}
          \State \textbf{continue}
        \Else
          \State \textbf{return} markedNt
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}

\subsection{Utilizing of spurious information}
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both algorithms~\ref{algo:verif1} and \ref{algo:verif2} take \markednewrons{} as input. 
The \markednewrons{} represent the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replaces the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as an abstractCEX, 
otherwise, return verified. Algorithm~\ref{algo:verif2} splits each neuron of \markednewrons{} into two sub-cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then the first case is when $x \in [lb,0]$, and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} runs for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
runs an exponential number of times in the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
the verification query is verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the abstractCEX. 


\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified abstractCEX. 
  \begin{algorithmic}[1]
    \State $constr := P \land (\Land_{i=1}^k lc_i)\land \neg Q$
    \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref*{eq:reluexact}}
    \State isSat = checkSat(constr)
    \If{isSat}
      \State \textbf{return} abstractCEX
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or  abstractCEX. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \State $constr := P \land (\Land_{i=1}^k lc'_i) \land \neg Q$ \Comment{$lc'$ is with respect to $A'$}
      \State isSat = checkSat(constr)
      \If{isSat}
        \State \textbf{return} abstractCEX
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}












%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
