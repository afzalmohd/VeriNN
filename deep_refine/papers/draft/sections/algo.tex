In this section, we present our method to refine \deeppoly in a novel way.
%
\texttt{DeepPoly} is a sound and incomplete technique because it does over-approximation analysis. 
If \deeppoly{} verifies the property then the property is guaranteed to be verified, otherwise, the result of it is unknown. 
We overcome this limitation by using a CEGAR-like technique, which is a complete technique
and rely on~\deeppoly{}. 

% \clearpage

% The goal is to find an input $\boldsymbol{x}$, such that the predicates $P(\boldsymbol{x})$ and 
% $Q(N(\boldsymbol{x}))$ holds. The predicate $Q$ usually is the negation of the desired property. 
% The triple  is our verification query.
%  \todo{How to formalize $P$ and $Q$}

An input vector $\boldsymbol{v} \in \reals^{|l_0|}$ is a counter-example(cex) if $N(\boldsymbol{v}) \models \lnot Q$. 
Let us say $satval_{ij}$ represents the satisfying value of variable $x_{ij}$, after an optimization query. 


% We will use the following convention in writing our algorithms.


% \begin{itemize}
% \item 
% \item 
% % \item $W_i, B_i$ represent the weight and bias matrix of layer $l_i$ respectively. 
% \end{itemize}


% \begin{df}
% \end{df}

Let us say $lc_i = \Land_{j=1}^{|l_i|} A(n_{ij}).lexpr \leq x_{ij} \leq  A(n_{ij}).uexpr$ is a 
conjunction of upper and lower constraints of each neuron of layer $l_i$ with respect to abstract constraint $A$.
The $lexpr$ and $uexpr$ for any neuron of a layer contain variables only from the previous layer's neurons, 
hence $lc_i$ contains the variables from layers $l_{i-1}$ and $l_i$. 

\begin{df}
  A sequence of value vectors $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$ is an 
  {\em abstract execution} of abstract constraint $A$ if 
  $\boldsymbol{v_0} \models lc_0$ and $\boldsymbol{v_{i-1}}, \boldsymbol{v_i} \models lc_i$ for each $i \in [1,k]$.  
 Moreover, an abstract execution $\boldsymbol{v_0,...,v_k}$ is
 an {\em abstract counter example(abstractCEX)} if $\boldsymbol{v_k} \models \lnot Q$.
\end{df}



The algorithm~\ref{algo:main} represents the high-level flow of our approach.
The first line of the algorithm generates all the abstract constraints by using \deeppoly{}. 
These abstract constraints are the lower and upper constraints as well as the lower and upper bounds 
of each neuron in the neural network. Algorithms \ref{algo:verif1} and \ref{algo:verif2} verify the query 
and return either the verification successful or abstractCEX. 

If these algorithms return verification successful then we report verified,
otherwise analyze the abstractCEX by algorithm \ref{algo:refine2} and return the causes of spuriousness (markedNeurons). 
Algorithm \ref{algo:verif1} exploits the markedNeurons to verify the property. 

Our approach contains two parts, the first part contains an approach to finding the markedNeurons. 
The second part contains the verification approach (utilizing markedNeurons).

\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State $A := deeppoly(N,P,Q)$\Comment{deeppoly generate the abstract constraints}
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or abstractCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{abstractCEX is a cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} cex
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons, abstractCEX)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\subsection{Causes of spuriousness} 
We have a following approach to find the causes of spuriousness (markedNeurons). 

\subsubsection{Optimization based approach: }

Let us say the abstractCEX is $\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$. 
Suppose $\boldsymbol{val_i^{\boldsymbol{v_0}}}$ represents the value vector on layer $l_i$, 
if we execute the neural network on input $\boldsymbol{v_0}$. 
The algorithm return $\boldsymbol{v_0}$ as a counter-example if $\boldsymbol{val_k^{\boldsymbol{v_0}}}$ 
does not satisfy the property $Q$. 
Since $l_0$ is an input layer, so, $\boldsymbol{v_0}$ and $\boldsymbol{val_0^{v_0}}$ are equal, 
and by theorem \ref{th:marked2} $\boldsymbol{v_1}$ and $\boldsymbol{val_1^{v_1}}$ are also equal. 
The core idea of this algorithm is to make $\boldsymbol{v'_2}$ as close as possible to $\boldsymbol{val_2^{v_0}}$, 
such that $\boldsymbol{v_0}, \boldsymbol{v_1}, \boldsymbol{v'_2}, ... \boldsymbol{v_k}$ become an abstractCEX. 
Here the closeness measured by the number of elements of $\boldsymbol{v'_2}$ becomes equal to the 
corresponding element of vector $\boldsymbol{val_2^{v_0}}$.
\begin{itemize}
  \item If $\boldsymbol{v'_2}$ becomes equal to $\boldsymbol{val_2^{v_0}}$ then $\boldsymbol{v'_3}$ will also become 
    equal to $\boldsymbol{val_3^{v_0}}$ by theorem \ref{th:marked2}. 
    Now we move on to the next relu layer $l_4$ and try to find the similar point $\boldsymbol{v'_4}$, such that 
    $\boldsymbol{v_0}, \boldsymbol{v_1}, \boldsymbol{v_2}, \boldsymbol{v'_4}... \boldsymbol{v_k}$ becomes an abstractCEX. 
    We repeat this process until the following case occurs. 
  \item If $\boldsymbol{v'_2}$ does not equal to $\boldsymbol{val_2^{v_0}}$ then we collect the neurons whose values differ, 
        and return them as a set of markedNeurons. 
\end{itemize}



\begin{theorem}
  \label{th:marked2}
  Let us say $\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$ is an abstract execution. 
  For an affine layer $l_i$, $\boldsymbol{val_i^{\boldsymbol{v_0}}} = \boldsymbol{v_i}$ if $\boldsymbol{val_{i-1}^{\boldsymbol{v_0}}} = \boldsymbol{v_{i-1}}$.  
  Since affine layer's constraints are exact.  
\end{theorem}
 

\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons \\
  \textbf{Input: } $\langle N,P,Q \rangle,A,markedNeurons,abstractCEX = \boldsymbol{v_0}, \boldsymbol{v_1} ... \boldsymbol{v_k}$\\
  \textbf{Output: } markedNeurons or real cex. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models \neg Q$. 
    \State Let us say $\boldsymbol{val_{i}}$ is the value vector at layer $l_i$, when $\boldsymbol{v_0}$ executed on network $N$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      \If{$l_i$ is $\relu${} layer}
        \State $constr := \Land_{t=i}^k lc_t$
        \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref{eq:reluexact}}
        \State $constr := constr \land \Land_{j=1}^{|l_{i-1}|} (x_{(i-1)j} = val_{i-1}(j))$
        \State $constr := constr \land \Land_{j=1}^{|l_k|} (x_{kj} = v_k(j))$
        \State $softConstrs := \cup_{j=1}^{|l_j|} (x_{ij} = val_i(j))$
        \State $res, softsatSet = maxsoftConstr(constr, softConstrs)$ \Comment{res always \texttt{SAT}}
        \State $markedNt := \{n_{ij} | 1 \leq j \leq |l_i| \land (x_{ij} = val_i(j)) \notin  softsatSet\}$ 
        \If{$markedNt$ is empty}
          \State \textbf{continue}
        \Else
          \State \textbf{return} markedNt
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}

\subsection{Utilizing of spurious information}
The \emph{isVerified} function in algorithm~\ref{algo:main} calls algorithm~\ref{algo:verif1}. 
The algorithm~\ref{algo:verif1} takes \markednewrons{} as input. 
The \markednewrons{} represent the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replaces the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as an abstractCEX, 
otherwise, return verified. 


\begin{algorithm}[t]
  \textbf{Name: } isVerified \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified abstractCEX. 
  \begin{algorithmic}[1]
    \State $constr := P \land (\Land_{i=1}^k lc_i)\land \neg Q$
    \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref*{eq:reluexact}}
    \State isSat = checkSat(constr)
    \If{isSat}
      \State \textbf{return} abstractCEX
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}


\subsection{Proofs of progress}
The theorem \ref{th:progress1} and \ref{th:progress2} implies that in every iteration {\em getMarkedNeurons} 
returns the non empty set of marked neurons, which are not already marked. In worst case the algorithms marked all 
the neurons of the network, and encode them in the exact behavior. 

Suppose the algorithm {\em getMarkedNeurons} get abstractCEX 
$\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$ as input, 
and $\boldsymbol{val_i^{\boldsymbol{v_0}}}$ represents the value vector on layer $l_i$, 
when we execute the neural network on input $\boldsymbol{v_0}$.
Suppose $\boldsymbol{satval_i}$ represent the satisfying assignment on layer $l_i$ after calling the optimization 
query at line 10 of algorithm \ref{algo:refine2}. 



\begin{theorem}
  \label{th:progress1}
  In every refinement iteration {\em getMarkedNeurons} return the non empty set of marked neurons. 
\end{theorem}

{\em Proof by contradiction:} 
By the definition of abstractCEX, $\boldsymbol{v_k} \models \lnot Q$. 
By the check at line 8 of algorithm~\ref{algo:main} $\boldsymbol{val_k^{\boldsymbol{v_0}}} \models Q$. 

Set of mark neurons is empty, implies that $markedNt = \{\}$ for each layer. 
$markedNt= \{\}$ implies that all the neurons in any layer $l_i$ become equal to $\boldsymbol{val_i^{\boldsymbol{v_0}}}$,  
which implies $\boldsymbol{v_k}$ equals to $\boldsymbol{val_k^{\boldsymbol{v_0}}}$, but $\boldsymbol{v_k} \models \lnot Q$ and 
$\boldsymbol{val_k^{\boldsymbol{v_0}}} \models Q$. Which is a contradiction. 


\begin{theorem}
  \label{th:progress2}
  In every refinement iteration {\em getMarkedNeurons} return the marked neurons which did not get mark in the previous iterations. 
\end{theorem}

{\em Proof: } Moreover, if a neurons $n_{ij}$ got marked in $t^{th}$ iteration then $n_{ij}$ will not marked again 
in any iteration greater than $t$.

Consider an iteration $t' > t$, if we get the marked neurons from layer other than $l_i$ then $n_{ij}$ 
can not be part of it because $n_{ij}$ is in layer $l_i$. 

Consider the case where marked neurons are from the layer $l_i$ in $t'^{th}$ iteration. 
Since we have repaired $n_{ij}$ in line 6 of algorithm \ref{algo:refine2}, its behavior in constraints 
optimization will be same as the exact relu. 
Moreover, $\boldsymbol{satval_i}(j) = \boldsymbol{val_i^{\boldsymbol{v_0}}}(j)$, 
which implies the soft constraint for neuron $n_{ij}$ will always be satisfied. Hence it will not occurs in 
marked neurons as per the criteria of marked neurons in line 11 of algorithm~\ref{algo:refine2}. 
 











%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:

