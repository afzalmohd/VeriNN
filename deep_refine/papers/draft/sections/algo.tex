%\clearpage
A {\em predicate} is a Boolean combination of $Linconstr$.
Let $P$ and $Q$ be a predicate over input and output layers respectively.
A verification query is a triple $\langle N, P, Q \rangle$.
We need to prove that for each input $\boldsymbol{v}$,
if $\boldsymbol{v} \models P$, $N(\boldsymbol{v}) \models Q$.
We assume $P$ is of the form
% Let us say $P$ is a predicate on input layer's variables
$\Land_{i=1}^{|l_0|}lb_i \leq x_{0i} \leq ub_i$, where $lb_i$ and $ub_i$ are lower and upper bounds respectively for a neuron $n_{0i}$.
An input vector $\boldsymbol{v} \in \reals^{|l_0|}$ is a counter-example(cex) if $N(\boldsymbol{v}) \models \lnot Q$. 
Let us say $satval_{ij}$ represents the satisfying value of variable $x_{ij}$, after an optimization query. 

% The goal is to find an input $\boldsymbol{x}$, such that the predicates $P(\boldsymbol{x})$ and 
% $Q(N(\boldsymbol{x}))$ holds. The predicate $Q$ usually is the negation of the desired property. 
% The triple  is our verification query.
%  \todo{How to formalize $P$ and $Q$}




% We will use the following convention in writing our algorithms.


% \begin{itemize}
% \item 
% \item 
% % \item $W_i, B_i$ represent the weight and bias matrix of layer $l_i$ respectively. 
% \end{itemize}


% \begin{df}
% \end{df}

Let us say $lc_i = \Land_{j=1}^{|l_i|} A(n_{ij}).lexpr \leq x_{ij} \leq  A(n_{ij}).uexpr$ is a 
conjunction of upper and lower constraints of each neuron of layer $l_i$ with respect to abstract constraint $A$.
The $lexpr$ and $uexpr$ for any neuron of a layer contain variables only from the previous layer's neurons, 
hence $lc_i$ contains the variables from layers $l_{i-1}$ and $l_i$. 

\begin{df}
  A sequence of value vectors $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$ is an 
  {\em abstract execution} of abstract constraint $A$ if 
  $\boldsymbol{v_0} \models lc_0$ and $\boldsymbol{v_{i-1}}, \boldsymbol{v_i} \models lc_i$ for each $i \in [1,k]$.  
 Moreover, an abstract execution $\boldsymbol{v_0,...,v_k}$ is
 an {\em abstract counter example(abstractCEX)} if $\boldsymbol{v_k} \models \lnot Q$.
\end{df}



\texttt{DeepPoly} is a sound and incomplete technique because it does over-approximation analysis. 
If \deeppoly{} verifies the property then the property is guaranteed to be verified, otherwise, the result of it is unknown. 
We overcome this limitation by using a cegar-based technique, which is a complete technique and rely on \deeppoly{}. 
The algorithm~\ref{algo:main} represents the high-level flow of our approach.
The first line of the algorithm generates all the abstract constraints by using \deeppoly{}. 
These abstract constraints are the lower and upper constraints as well as the lower and upper bounds 
of each neuron in the neural network. Algorithms \ref{algo:verif1} and \ref{algo:verif2} verify the query 
and return either the verification successful or abstractCEX. If these algorithms return verification successful then we report verified,
otherwise analyze the abstractCEX by algorithms \ref{algo:refine1} and \ref{algo:refine2} and return the causes of spuriousness (markedNeurons). 
Algorithms \ref{algo:verif1} and \ref{algo:verif2} exploit the markedNeurons to verify the property. 

Our approach contains two parts, the first part contains the approaches to finding the markedNeurons. 
The second part contains the verification approaches (utilizing spurious information).

\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State $A := deeppoly(N,P,Q)$\Comment{deeppoly generate the abstract constraints}
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or abstractCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{abstractCEX is a cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} cex
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\subsection{Causes of spuriousness} 
We have two approaches to find the causes of spuriousness (markedNeurons). 

\subsubsection{Pullback approach: }

Suppose the abstractCEX is $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$. 
The core idea of this approach is to find a point $\boldsymbol{p_{k-1}}$ in the layer $l_{k-1}$. 
The point $\boldsymbol{v_k}$ is guaranteed to be reachable from point $\boldsymbol{p_{k-1}}$ in the concrete domain.
Similarly, we find points $\boldsymbol{p_{k-3}}, \boldsymbol{p_{k-5}}, ... \boldsymbol{p_0}$, 
such that $p_i$ is always reachable from point $p_{i-2}$. 
We find these points on each \relu{} layer and input layer only. 
If we find the point $\boldsymbol{p_0}$ in the input layer, it means $\boldsymbol{p_0}$ is a counter-example, 
because we can reach from $\boldsymbol{p_0}$ to $\boldsymbol{p_2}$, $\boldsymbol{p_2}$ to $\boldsymbol{p_4}$ 
, and so on up to $\boldsymbol{v_k}$. 
If we get stuck in some layer $l_i$ i.e. fails to find point 
$\boldsymbol{p_{i-2}}$. It means point $\boldsymbol{p_i}$ does not have its corresponding point $\boldsymbol{p_{i-2}}$, 
which implies that point $\boldsymbol{p_i}$ is a spurious point generated by \relu{} layer $l_i$. 
The algorithm \ref{algo:refine1} compute such points. In line number $2$, we equate the value of 
each element of $\boldsymbol{v_k}$ to the corresponding neuron's affine expression($lexpr$ or $uexpr$), 
and take the conjunction, and check satisfiability. Since the affine expression of each neuron in $l_k$ contains the 
variable of layer $l_{i-1}$, so, the satisfying assignment is the point $p_{k-1}$. Similarly, we build the constraints
for each hidden affine layer's neurons. For a neuron $n_{ij}$ of affine layer $l_i$, 
if the corresponding point's value $p_{i+1}(j)$ is greater than $0$ then we equate the affine expression of $x_{ij}$ to $p_{i+1}(j)$,
otherwise, we set the lower and upper bound of the affine expression of $x_{ij}$ as $A(x_{ij}).lb$ and $0$ respectively. 
Which is the replication of \relu{} function $x_{(i+1)j} = max(0, x_{ij})$. We construct such constraint 
for each neuron of $l_i$, and build a formula by taking the conjunction
of each neuronâ€™s constraint and checking the satisfiability of this formula.
If it is satisfiable then the point $\boldsymbol{p_{i-1}}$ is found, 
otherwise, we get the $\mathtt{unsat}$core. We collect all the neurons of $l_i$ whose constraints are 
in $\mathtt{unsat}$core, and return them as the markedNeurons.   

\begin{algorithm}[t]
  \textbf{Name: } pullback \\
  \textbf{Input: } $\langle N,P,Q \rangle$, abstract constraints $A$ and abstractCEX $=\boldsymbol{v_0}, \boldsymbol{v_1}, .., \boldsymbol{v_k}$ \\
  \textbf{Output: } markedNeurons or cex. 
  \begin{algorithmic}[1]
   \State \textbf{return} $\boldsymbol{v_0}$ if $N(\boldsymbol{v_0}) \models \neg Q$. 
   \State $constr := \Land_{j=1}^{|l_k|} (A(x_{kj}).lexpr = v_k(j))$
   \State isSat = checkSat(constr)
   \If{isSat} \Comment{must be true, last affine layer dont add spurious information}
      \State $\boldsymbol{p_{k-1}} = \boldsymbol{satval_{k-1}}$ 
   \EndIf
   \For{$i=k-1$ to $1$}
      \If{$l_i$ is affine layer}
        \State $layerConstraints := true$
        \For{$j=1$ to $|l_i|$}
          \If{$p_{i+1}(j) > 0$}
            \State $constr_{ij}$ := $(A(x_{ij}).lexpr = p_{i+1}(j)$) \Comment{lexpr=uexpr for affine}
          \Else
            \State $constr_{ij}$ := $(A(x_{ij}).lb \leq A(n_{ij}).lexpr \leq 0$)
          \EndIf
          \State $layerConstrains := layerConstrains \land constr_{ij}$
        \EndFor
        \State isSat = checkSat(layerConstrains)
        \If{not isSat}
          \State markedNeurons = \{$n_{ij}$ | $1 \leq j\leq |l_i| \land constr_{ij}$ $\in$ unsatCore\}
          \State \textbf{return } markedNeurons
        \Else
          \State $\boldsymbol{p_{i-1}} = \boldsymbol{satval_{i-1}}$
        \EndIf
      \EndIf
   \EndFor
    \State \textbf{return} $\boldsymbol{p_0}$ \Comment{cex if pullbacked till input layer}
  \end{algorithmic}
  \caption{A pullback approach to get mark neurons or counter example}
  \label{algo:refine1}
\end{algorithm}

\subsubsection{Optimization based approach: }

Let us say the abstractCEX is $\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$. 
The algorithm return $\boldsymbol{v_0}$ as a counter-example if $\boldsymbol{val_k^{\boldsymbol{v_0}}}$ 
does not satisfy the property $Q$. 
Since $l_0$ is an input layer, so, $\boldsymbol{v_0}$ and $\boldsymbol{val_0^{v_0}}$ are equal, 
and by theorem \ref{th:marked2} $\boldsymbol{v_1}$ and $\boldsymbol{val_1^{v_1}}$ are also equal. 
The core idea of this algorithm is to make $\boldsymbol{v'_2}$ as close as possible to $\boldsymbol{val_2^{v_0}}$, 
such that $\boldsymbol{v_0}, \boldsymbol{v_1}, \boldsymbol{v'_2}, ... \boldsymbol{v_k}$ become an abstractCEX. 
Here the closeness measured by the number of elements of $\boldsymbol{v'_2}$ becomes equal to the 
corresponding element of vector $\boldsymbol{val_2^{v_0}}$.
\begin{itemize}
  \item If $\boldsymbol{v'_2}$ becomes equal to $\boldsymbol{val_2^{v_0}}$ then $\boldsymbol{v'_3}$ will also become 
    equal to $\boldsymbol{val_3^{v_0}}$ by theorem \ref{th:marked2}. 
    Now we move on to the next relu layer $l_4$ and try to find the similar point $\boldsymbol{v'_4}$, such that 
    $\boldsymbol{v_0}, \boldsymbol{v_1}, \boldsymbol{v_2}, \boldsymbol{v'_4}... \boldsymbol{v_k}$ becomes an abstractCEX. 
    We repeat this process until the following case occurs. 
  \item If $\boldsymbol{v'_2}$ does not equal to $\boldsymbol{val_2^{v_0}}$ then we collect the neurons whose values differ, 
        and return them as a set of markedNeurons. 
\end{itemize}



\begin{theorem}
  \label{th:marked2}
  Let us say $\boldsymbol{v_0}, \boldsymbol{v_1}, ... \boldsymbol{v_k}$ is an abstract execution. 
  For an affine layer $l_i$, $\boldsymbol{val_i^{\boldsymbol{v_0}}} = \boldsymbol{v_i}$ if $\boldsymbol{val_{i-1}^{\boldsymbol{v_0}}} = \boldsymbol{v_{i-1}}$.  
  Since affine layer's constraints are exact.  
\end{theorem}
 

\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons2 \\
  \textbf{Input: } $\langle N,P,Q \rangle,A,markedNeurons,abstractCEX = \boldsymbol{v_0}, \boldsymbol{v_1} ... \boldsymbol{v_k}$\\
  \textbf{Output: } markedNeurons or real cex. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models \neg Q$. 
    \State Let us say $\boldsymbol{val_{i}}$ is the value vector at layer $l_i$, when $\boldsymbol{v_0}$ executed on network $N$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      \If{$l_i$ is \relu{} layer}
        \State $constr := \Land_{t=i}^k lc_t$
        \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref{eq:reluexact}}
        \State $constr := constr \land \Land_{j=1}^{|l_{i-1}|} (x_{(i-1)j} = val_{i-1}(j))$
        \State $constr := constr \land \Land_{j=1}^{|l_k|} (x_{kj} = v_k(j))$
        \State $softConstrs := \cup_{j=1}^{|l_j|} (x_{ij} = val_i(j))$
        \State $res, softsatSet = maxsoftConstr(constr, softConstrs)$ \Comment{res always \texttt{SAT}}
        \State $markedNt := \{n_{ij} | 1 \leq j \leq |l_i| \land (x_{ij} = val_i(j)) \notin  softsatSet\}$ 
        \If{$markedNt$ is empty}
          \State \textbf{continue}
        \Else
          \State \textbf{return} markedNt
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}

\subsection{Utilizing of spurious information}
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both algorithms~\ref{algo:verif1} and \ref{algo:verif2} take \markednewrons{} as input. 
The \markednewrons{} represent the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replaces the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as an abstractCEX, 
otherwise, return verified. Algorithm~\ref{algo:verif2} splits each neuron of \markednewrons{} into two sub-cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then the first case is when $x \in [lb,0]$, and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} runs for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
runs an exponential number of times in the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
the verification query is verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the abstractCEX. 


\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified abstractCEX. 
  \begin{algorithmic}[1]
    \State $constr := P \land (\Land_{i=1}^k lc_i)\land \neg Q$
    \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref*{eq:reluexact}}
    \State isSat = checkSat(constr)
    \If{isSat}
      \State \textbf{return} abstractCEX
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or  abstractCEX. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \State $constr := P \land (\Land_{i=1}^k lc'_i) \land \neg Q$ \Comment{$lc'$ is with respect to $A'$}
      \State isSat = checkSat(constr)
      \If{isSat}
        \State \textbf{return} abstractCEX
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}












%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
