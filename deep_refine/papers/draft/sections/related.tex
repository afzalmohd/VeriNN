 The papers also do the refinement on \deeppoly{} are \cite{yang2021improving} and \cite{singh2019beyond}, 
 although these papers do not do the cegar based refinement. The approach \cite{singh2019beyond} consider a group neurons
 at once to generate the constraints and compute the bounds of neurons. The approach \cite{yang2021improving} remove the 
 spurious region by taking each neuron's constrains with the negation of the property and use the 
 \texttt{Gurobi}~\cite{gurobioptimizer} optimizer. 
 As per our knowledge elbohar et al~\cite{elboher2020abstraction} and ~\cite{liu2022abstraction} do the cegar based refinement, 
 but their abstraction techniques orthogonal to \deeppoly{}. They reduces the network size by merging the similar 
 neurons with over-approximation, while \deeppoly{} maintains the linear constrains for each neurons without changing the 
 structure of the network. Paper~\cite{lin2020art} do the refinement by bisecting the input space on the guided dimension. 
 
 In general neural network verification techniques can be classify broadly in two categories, 
 complete and incomplete techniques. The techniques \cite{lomuscio2017approach}, \cite{fischetti2018deep},
 \cite{dutta2018output}, \cite{cheng2017maximum}, \cite{katz2017reluplex}, \cite{katz2019marabou}, 
 \cite{ehlers2017formal}, \cite{huang2017safety}, \cite{wang2021beta}, \cite{xu2020fast}, \cite{zhang2022general}
 are the complete techniques. Techniques which either return \texttt{verified} or a counter example 
 are known as complete techniques. 
 The complete techniques explore the exact state space, hence, they suffers with scalability issues on the large 
 size of networks. On the other hand, the techniques \cite{dvijotham2018dual}, \cite{gehr2018ai2}, \cite{singh2018fast},
 \cite{singh2018boosting}, \cite{weng2018towards}, \cite{wong2018provable}, \cite{zhang2018efficient}, \cite{zhang2018efficient}
 are the sound and incomplete techniques. The incomplete techniques usually over approximate the state space, hence, 
 these techniques scale well but do not remain complete. Our approach exploit the incomplete technique \deeppoly{},
 which scale well, but if it fails to find the counter example then we do the cegar based refinement.   