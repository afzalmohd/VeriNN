We have implemented our approach in a prototype and compared it with the related 
approaches \texttt{refinepoly}~\cite{singh2019beyond}, \texttt{deepSRGR}~\cite{yang2021improving}, and \deeppoly{}~\cite{singh2019abstract}. 
The approaches  \texttt{refinepoly} and \texttt{deepSRGR} do the refinement on \deeppoly{}. We also compare 
our tool with state-of-the-art tools $\alpha - \beta$-Crown~\cite{alphabetacrown} and oval~\cite{ovaltool}. 
The $\alpha - \beta$-Crown tool has achieved the 1st and oval 3rd rank in
2nd International Verification of Neural Networks Competition (VNN-COMP'21). 
We use the MNIST \cite{deng2012mnist} for our evaluation.    
\subsection{Implementation}
We have implemented our tool in \texttt{C++} programming language. Our approach relies on \deeppoly{}, so, 
we also have implemented \deeppoly{} in \texttt{C++}. We are using a \texttt{C++} interface of gurobi solver 
to check the satisfiability or to solve an optimization query. 

\subsection{Benchmarks}
We are using the MNIST~\cite{deng2012mnist} dataset to check the effectiveness of our tool. 
We are using 11 different fully connected feedforward neural networks with $\relu${} activation as shown in table~\ref{tb:nndetail}.
These benchmarks are taken from the \deeppoly{}'s paper~\cite{singh2019abstract}. 
The input and output dimensions of each network are $784$ and $10$ respectively. 
The authors of \deeppoly{} used projected gradient descent (PGD)~\cite{dong2018boosting}
and DiffAI~\cite{mirman2018differentiable} for adversarial training. Table \ref{tb:nndetail} contains the defended network i.e.
trained with adversarial training as well as the undefended network. The last column of the table \ref{tb:nndetail}
shows tool's name by which the defended networks are trained.  

The predicate $P$ on the input layer is created using the input image $\boldsymbol{im}$ and user-defined parameter $\epsilon$. 
We first normalize each pixel of $\boldsymbol{im}$ between $0$ and $1$, then create 
$P = \Land_{i=1}^{|l_0|} im(i)-\epsilon \leq x_{0i}\leq im(i)+\epsilon$, such that the lower and upper bound of each pixel
should not exceed $0$ and $1$ respectively. The predicate $Q$ on the output layer is created using the network's output.    
Suppose the predicted label of $\boldsymbol{im}$ on network $N$ is $y$, then $Q = \Land_{i=1}^{|l_k|} x_{ki} < y$, where $i \neq y$. 
One query instance $\langle N,P,Q \rangle$ is created for one network, one image and one epsilon value. 
In our evaluation, we took $11$ different networks, 8 different epsilons, and 100 different images. The 
total number of instances is computed to $8800$. However, there are 128 instances for which the network's predicted
label differs from the image's actual label. we avoided such instances, so, there is a total of $8672$ benchmark instances
under consideration.   
Whenever our tool found a counter-example on $\langle N,P,Q \rangle$,
it denormalizes it into an image by rounding the float values 
and checks for counter-example by executing $N$ on the denormalized image.
If found counter-example then the tool reports it, otherwise, the tool reports unknown.



\begin{table}
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        \textbf{Neural Network} & \textbf{\#hidden layers} & \textbf{\#activation units} & \textbf{Defensive training} \\
        \hline
        $3\times 50$ & 2 & 110 & None \\
        $3\times 100$ & 2 & 210 & None  \\
        $5\times 100$ & 4 & 410 & None  \\
        $6\times 100$ & 5 & 510 & DiffAI \\
        $9\times 100$ & 8 & 810 & None  \\
        $6\times 200$ & 5 & 1010 & None  \\
        $9\times 200$ & 8 & 1610 & None  \\
        $6\times 500$ & 6 & 3000 & None  \\
        $6\times 500$ & 6 & 3000 & PGD, $\epsilon = 0.1$ \\
        $6\times 500$ & 6 & 3000 & PGD, $\epsilon = 0.3$ \\
        $4\times 1024$ & 3 & 3072 & None  \\
        \hline
    \end{tabular}
    \caption{Neural networks details}
    \label{tb:nndetail}
\end{table}

\subsection{Results}
We conducted the experiments on a machine with \texttt{64GB RAM, 2.20 GHz Intel(R) Xeon(R) CPU E5-2660 v2}
processor with CentOS Linux 7 operating system. 
To make a fair comparison between the tools, we provide only a single \texttt{CPU} for each benchmarks instances for each tool. 
We have two plots~\ref{res:milp_with_milp}, ~\ref{res:ep:milp_with_milp}, and one
table~\ref{tb:matrix} in this section. The row of table represents the verified cases while the column
represents the not verified cases, i.e.\kpoly{}'th row and \deeppoly{}'th column represent the 
156 number of benchmarks instances which are solved by \kpoly{} and not solved by \deeppoly{}. 
We noticed that we are marking average $9.8\%$ of neurons, whenever we need to mark.     

\subsubsection{Comparison with the most related techniques:}
In this section we consider the techniques \deeppoly{}, \kpoly{} and \deepsrgr{} to compare with
our technique. 
We consider \deeppoly{} because it is a base tool for our technique,  
and the techniques \kpoly{} and \deepsrgr{} do the refinement on top of \deeppoly{}.
Since these technique's tool reports only \texttt{verified} instances, but our tool can report 
\texttt{verified} as well as counter-example.  
So, we compare these techniques with only \texttt{verified} 
instances of our technique in the cactus plot ~\ref{res:milp_with_milp}. 
Our technique performs well in terms of the verified number of instances as well as in terms 
of time in comparison to these three techniques. 
It can be shown in table~\ref{tb:matrix} that our tool solves all the benchmark instances 
which are solved by these three techniques, except 
the $14$ instances. These $14$ instances are solved by \kpoly{}, but our tool goes timeout for them. 

% The approaches \deeppoly{}, \texttt{refinepoly}, and \texttt{deepSRGR} are incomplete and do not report the counter-example. 
% Our approach is a complete approach. We compare our approach's \texttt{verified} only instances with incomplete approaches. 
% Figure~\ref{res:milp_with_milp} shows the comparison with the help of the cactus plot. Our approach performs well 
% in terms of the verified number of instances as well as in terms of time in comparison to the above incomplete approaches. 
% We are also comparing with state of the arts to check the effectiveness of our approach globally.
% Table~\ref{tb:soacomparison} shows that our tool is verifying $180$ and $190$ benchmarks which $\alpha -\beta$-CROWN and 
% \texttt{oval} respectively are not able to verify. In addition to this, our tool verifies $172$ unique instances 
% that neither $\alpha -\beta$-CROWN nor \texttt{oval} is able to verify.  
% Despite it, $\alpha - \beta-$Crown and \texttt{oval} are verifying more benchmarks in comparison to our tool. 
% Although the total time taken by both state of the arts is higher than our tool as shown in figure~\ref{res:milp_with_milp}. 


% \begin{table}
%     \centering
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         -  & Not verified by oval & Not verified by $\alpha - \beta$-CROWN & Not verified by both \\
%         \hline
%         Verified by our tool & 190 & 180 & 172 \\
%         \hline
%     \end{tabular}
%     \caption{Comparison with $\alpha - \beta$-CROWN and Oval}
%     \label{tb:soacomparison}
% \end{table}



\begin{figure}
    \centering
    \input{fig/data_plot.tex}
    \caption{Cactus plot with related techniques}
    \label{res:milp_with_milp}
\end{figure}

\subsubsection{Comparison with cegar based technique: }
As per our knowledge, \texttt{cagar\_nn}~\cite{elboher2020abstraction} is the only published work which do 
counter example guided refinement. But they use the different abstraction in comparison to the 
\deeppoly{}. They reduce the size of the network by merging the similar neurons, such that 
they maintain the overapproximation. And split back in the refinement process. 
We can see in figure~\ref{res:milp_with_milp} that \texttt{cegar\_nn} could verified only 
$18.88\%$ while our tool verified $61.42\%$ of the total number of benchmark instances. 

\subsubsection{Comparison with portfolio state of the arts: }
todo\todo{todo}

\subsubsection{Epsilon wise performance: }
Figure~\ref{res:ep:milp_with_milp} shows the comparison with respect to different epsilon values. 
At $\epsilon=0.005$, the performance of all the tools is almost the same. As the value of epsilon increases, the 
performance of tools can be seen clearly. Here also we are performing better than the approaches \deeppoly{}, 
\texttt{refinepoly}, and \texttt{deepSRGR},
while $\alpha - \beta -$Crown and \texttt{oval} are performing better than our tool. 

% Figures \ref{res:milp_with_milp} and \ref{res:ep:milp_with_milp} shows that we are performing better in compare to
% the most related techniques \deeppoly{}, \texttt{refinepoly} and \texttt{deepSRGR}. 

\begin{figure}
    \centering
    \input{fig/ep_plot.tex}
    \caption{Epsilon wise comparison with related techniques}
    \label{res:ep:milp_with_milp}
\end{figure}


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        - & \textbf{total} & \textbf{cegar\_nn} & \textbf{deeppoly} & \textbf{kPoly} & \textbf{deepSRGR} & \textbf{abcrown} & \textbf{oval} & \textbf{drefine} \\
        \hline
        cegar\_nn & 1638 & 0 & 713 & 609 & 687 & 217 & 238 & 417 \\ 
        \hline
        deeppoly & 4633 & 3708 & 0 & 0 & 0 & 63 & 66 & 0  \\ 
        \hline
        kPoly & 4789 & 3760 & 156 & 0 & 114 & 63 & 66 & 14  \\ 
        \hline
        deepSRGR & 4687 & 951 & 54 & 2 & 0 & 63 & 66 & 0 \\ 
        \hline
        abcrown & 6242 & 4821 & 1672 & 1516 & 1618 & 0 & 84 & 1095  \\
        \hline
        oval & 6189 & 4789 & 1622 & 1466 & 1568 & 31 & 0 & 1052 \\
        \hline
        drefine & 5327 & 4106 & 694 & 552 & 640 & 180 & 190 & 0  \\
        \hline
    \end{tabular}
    \caption{Neural networks details}
    \label{tb:matrix}
\end{table}