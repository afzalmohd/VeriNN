We have implemented our approach in a prototype and compared it with the related 
approaches \texttt{refinepoly}~\cite{singh2019beyond}, \texttt{deepSRGR}~\cite{yang2021improving}, and \deeppoly{}~\cite{singh2019abstract}. 
The approaches  \texttt{refinepoly} and \texttt{deepSRGR} do the refinement on \deeppoly{}. We also compare 
our tool with state-of-the-art tools $\alpha - \beta$-Crown~\cite{alphabetacrown} and oval~\cite{ovaltool}. 
The $\alpha - \beta$-Crown tool has achieved the 1st and oval 3rd rank in
2nd International Verification of Neural Networks Competition (VNN-COMP'21). 
We use the MNIST \cite{deng2012mnist} for our evaluation.    
\subsection{Implementation}
We have implemented our tool in \texttt{C++} programming language. Our approach relies on \deeppoly{}, so, 
we also have implemented \deeppoly{} in \texttt{C++}. We are using a \texttt{C++} interface of gurobi solver 
to check the satisfiability or to solve an optimization query. 

\subsection{Benchmarks}
We are using the MNIST~\cite{deng2012mnist} dataset to check the effectiveness of our tool. 
We are using 11 different fully connected feedforward neural networks with \relu{} activation as shown in table~\ref{tb:nndetail}.
These benchmarks are taken from the \deeppoly{}'s paper~\cite{singh2019abstract}. 
The input and output dimensions of each network are $784$ and $10$ respectively. 
The authors of \deeppoly{} used projected gradient descent (PGD)~\cite{dong2018boosting}
and DiffAI~\cite{mirman2018differentiable} for adversarial training. Table \ref{tb:nndetail} contains the defended network i.e.
trained with adversarial training as well as the undefended network. The last column of the table \ref{tb:nndetail}
shows tool's name by which the defended networks are trained.  

The predicate $P$ on the input layer is created using the input image $\boldsymbol{im}$ and user-defined parameter $\epsilon$. 
We first normalize each pixel of $\boldsymbol{im}$ between $0$ and $1$, then create 
$P = \Land_{i=1}^{|l_0|} im(i)-\epsilon \leq x_{0i}\leq im(i)+\epsilon$, such that the lower and upper bound of each pixel
should not exceed $0$ and $1$ respectively. The predicate $Q$ on the output layer is created using the network's output.    
Suppose the predicted label of $\boldsymbol{im}$ on network $N$ is $y$, then $Q = \Land_{i=1}^{|l_k|} x_{ki} < y$, where $i \neq y$. 
One query instance $\langle N,P,Q \rangle$ is created for one network, one image and one epsilon value. 
In our evaluation, we took $11$ different networks, 8 different epsilons, and 100 different images. The 
total number of instances is computed to $8800$. However, there are 128 instances for which the network's predicted
label differs from the image's actual label. we avoided such instances, so, there is a total of $8672$ benchmark instances
under consideration.   
Whenever our tool found a counter-example on $\langle N,P,Q \rangle$,
it denormalizes it into an image by rounding the float values 
and checks for counter-example by executing $N$ on the denormalized image.
If found counter-example then the tool reports it, otherwise, the tool reports unknown.



\begin{table}
    \centering
    \begin{tabular}{c|c|c|c}
        \hline
        \textbf{Neural Network} & \textbf{\#hidden layers} & \textbf{\#activation units} & \textbf{Defensive training} \\
        \hline
        $3\times 50$ & 2 & 110 & None \\
        $3\times 100$ & 2 & 210 & None  \\
        $5\times 100$ & 4 & 410 & None  \\
        $6\times 100$ & 5 & 510 & DiffAI \\
        $9\times 100$ & 8 & 810 & None  \\
        $6\times 200$ & 5 & 1010 & None  \\
        $9\times 200$ & 8 & 1610 & None  \\
        $6\times 500$ & 6 & 3000 & None  \\
        $6\times 500$ & 6 & 3000 & PGD, $\epsilon = 0.1$ \\
        $6\times 500$ & 6 & 3000 & PGD, $\epsilon = 0.3$ \\
        $4\times 1024$ & 3 & 3072 & None  \\
        \hline
    \end{tabular}
    \caption{Neural networks details}
    \label{tb:nndetail}
\end{table}

\subsection{Results}
We conducted the experiments on a machine with \texttt{64GB RAM, 2.20 GHz Intel(R) Xeon(R) CPU E5-2660 v2}
processor with CentOS Linux 7 operating system. 
To make a fair comparison between the tools, we provide only a single \texttt{CPU} for each benchmarks instances for each tool. 
The approaches \deeppoly{}, \texttt{refinepoly}, and \texttt{deepSRGR} are incomplete and do not report the counter-example. 
Our approach is a complete approach. We compare our approach's \texttt{verified} only instances with incomplete approaches. 
Figure~\ref{res:milp_with_milp} shows the comparison with the help of the cactus plot. Our approach performs well 
in terms of the verified number of instances as well as in terms of time in comparison to the above incomplete approaches. 
We are also comparing with state of the arts to check the effectiveness of our approach globally.
Table~\ref{tb:soacomparison} shows that our tool is verifying $180$ and $190$ benchmarks which $\alpha -\beta$-CROWN and 
\texttt{oval} respectively are not able to verify. In addition to this, our tool verifies $172$ unique instances 
that neither $\alpha -\beta$-CROWN nor \texttt{oval} is able to verify.  
Despite it, $\alpha - \beta-$Crown and \texttt{oval} are verifying more benchmarks in comparison to our tool. 
Although the total time taken by both state of the arts is higher than our tool as shown in figure~\ref{res:milp_with_milp}. 


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        -  & Not verified by oval & Not verified by $\alpha - \beta$-CROWN & Not verified by both \\
        \hline
        Verified by our tool & 190 & 180 & 172 \\
        \hline
    \end{tabular}
    \caption{Comparison with $\alpha - \beta$-CROWN and Oval}
    \label{tb:soacomparison}
\end{table}



\begin{figure}
    \centering
    \input{fig/data_plot.tex}
    \caption{Cactus plot with related techniques}
    \label{res:milp_with_milp}
\end{figure}


Figure~\ref{res:ep:milp_with_milp} shows the comparison with respect to different epsilon values. 
At $\epsilon=0.005$, the performance of all the tools is almost the same. As the value of epsilon increases, the 
performance of tools can be seen clearly. Here also we are performing better than the approaches \deeppoly{}, 
\texttt{refinepoly}, and \texttt{deepSRGR},
while $\alpha - \beta -$Crown and \texttt{oval} are performing better than our tool. 
The average number of marked neurons on the finished benchmarks instances is $31$. 

% Figures \ref{res:milp_with_milp} and \ref{res:ep:milp_with_milp} shows that we are performing better in compare to
% the most related techniques \deeppoly{}, \texttt{refinepoly} and \texttt{deepSRGR}. 

\begin{figure}
    \centering
    \input{fig/ep_plot.tex}
    \caption{Epsilon wise comparison with related techniques}
    \label{res:ep:milp_with_milp}
\end{figure}
