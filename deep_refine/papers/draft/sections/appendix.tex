% \onecolumn

\begin{center}
    \Large{\bf{Appendix}}    
\end{center}


\subsubsection{Pullback approach: }

Suppose the abstractCEX is $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$. 
The core idea of this approach is to find a point $\boldsymbol{p_{k-1}}$ in the layer $l_{k-1}$. 
The point $\boldsymbol{v_k}$ is guaranteed to be reachable from point $\boldsymbol{p_{k-1}}$ in the concrete domain.
Similarly, we find points $\boldsymbol{p_{k-3}}, \boldsymbol{p_{k-5}}, ... \boldsymbol{p_0}$, 
such that $p_i$ is always reachable from point $p_{i-2}$. 
We find these points on each $\relu${} layer and input layer only. 
If we find the point $\boldsymbol{p_0}$ in the input layer, it means $\boldsymbol{p_0}$ is a counter-example, 
because we can reach from $\boldsymbol{p_0}$ to $\boldsymbol{p_2}$, $\boldsymbol{p_2}$ to $\boldsymbol{p_4}$ 
, and so on up to $\boldsymbol{v_k}$. 
If we get stuck in some layer $l_i$ i.e. fails to find point 
$\boldsymbol{p_{i-2}}$. It means point $\boldsymbol{p_i}$ does not have its corresponding point $\boldsymbol{p_{i-2}}$, 
which implies that point $\boldsymbol{p_i}$ is a spurious point generated by $\relu${} layer $l_i$. 
The algorithm \ref{algo:refine1} compute such points. In line number $2$, we equate the value of 
each element of $\boldsymbol{v_k}$ to the corresponding neuron's affine expression($lexpr$ or $uexpr$), 
and take the conjunction, and check satisfiability. Since the affine expression of each neuron in $l_k$ contains the 
variable of layer $l_{i-1}$, so, the satisfying assignment is the point $p_{k-1}$. Similarly, we build the constraints
for each hidden affine layer's neurons. For a neuron $n_{ij}$ of affine layer $l_i$, 
if the corresponding point's value $p_{i+1}(j)$ is greater than $0$ then we equate the affine expression of $x_{ij}$ to $p_{i+1}(j)$,
otherwise, we set the lower and upper bound of the affine expression of $x_{ij}$ as $A(x_{ij}).lb$ and $0$ respectively. 
Which is the replication of $\relu${} function $x_{(i+1)j} = max(0, x_{ij})$. We construct such constraint 
for each neuron of $l_i$, and build a formula by taking the conjunction
of each neuronâ€™s constraint and checking the satisfiability of this formula.
If it is satisfiable then the point $\boldsymbol{p_{i-1}}$ is found, 
otherwise, we get the $\mathtt{unsat}$core. We collect all the neurons of $l_i$ whose constraints are 
in $\mathtt{unsat}$core, and return them as the markedNeurons.   



\begin{algorithm}[t]
    \textbf{Name: } pullback \\
    \textbf{Input: } $\langle N,P,Q \rangle$, abstract constraints $A$ and abstractCEX $=\boldsymbol{v_0}, \boldsymbol{v_1}, .., \boldsymbol{v_k}$ \\
    \textbf{Output: } markedNeurons or cex. 
    \begin{algorithmic}[1]
     \State \textbf{return} $\boldsymbol{v_0}$ if $N(\boldsymbol{v_0}) \models \neg Q$. 
     \State $constr := \Land_{j=1}^{|l_k|} (A(x_{kj}).lexpr = v_k(j))$
     \State isSat = checkSat(constr)
     \If{isSat} \Comment{must be true, last affine layer dont add spurious information}
        \State $\boldsymbol{p_{k-1}} = \boldsymbol{satval_{k-1}}$ 
     \EndIf
     \For{$i=k-1$ to $1$}
        \If{$l_i$ is affine layer}
          \State $layerConstraints := true$
          \For{$j=1$ to $|l_i|$}
            \If{$p_{i+1}(j) > 0$}
              \State $constr_{ij}$ := $(A(x_{ij}).lexpr = p_{i+1}(j)$) \Comment{lexpr=uexpr for affine}
            \Else
              \State $constr_{ij}$ := $(A(x_{ij}).lb \leq A(n_{ij}).lexpr \leq 0$)
            \EndIf
            \State $layerConstrains := layerConstrains \land constr_{ij}$
          \EndFor
          \State isSat = checkSat(layerConstrains)
          \If{not isSat}
            \State markedNeurons = \{$n_{ij}$ | $1 \leq j\leq |l_i| \land constr_{ij}$ $\in$ unsatCore\}
            \State \textbf{return } markedNeurons
          \Else
            \State $\boldsymbol{p_{i-1}} = \boldsymbol{satval_{i-1}}$
          \EndIf
        \EndIf
     \EndFor
      \State \textbf{return} $\boldsymbol{p_0}$ \Comment{cex if pullbacked till input layer}
    \end{algorithmic}
    \caption{A pullback approach to get mark neurons or counter example}
    \label{algo:refine1}
  \end{algorithm}



\subsection{Utilizing of spurious information}
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both algorithms~\ref{algo:verif1} and \ref{algo:verif2} take \markednewrons{} as input. 
The \markednewrons{} represent the set of the culprit neurons. 
Algorithm~\ref{algo:verif2} splits each neuron of \markednewrons{} into two sub-cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then the first case is when $x \in [lb,0]$, and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} runs for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
runs an exponential number of times in the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
the verification query is verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the abstractCEX. 

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or  abstractCEX. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \State $constr := P \land (\Land_{i=1}^k lc'_i) \land \neg Q$ \Comment{$lc'$ is with respect to $A'$}
      \State isSat = checkSat(constr)
      \If{isSat}
        \State \textbf{return} abstractCEX
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}




\textbf{Pullback approach}\\
% The core idea of this approach is to find a point in the predecessor layer, such that the current point 
% can be reached from the predecessor layers point. 
This approach finds a point of $x_7=2,x_8=1$ in the predecessor layer means the value of $x_5$ and $x_6$. 
The point $x_7=2$ and $x_8=1$ must reach from the values of $x_5$ and $x_6$. 
We compute the value by using the \sat{} query on the affine layer. The \sat{} query shown 
in equation~\ref{eq:back1}, 
build by using the values and affine expression of $x_7$, $x_8$, and by using the bounds of $x_5$ and $x_6$.
The satisfying value of equation~\ref{eq:back1} is $x_5=2$ and $x_6=0$, which is the predecessor point of $x_7=2$
and $x_8=1$.  
\begin{equation}
    \begin{aligned}
        x_7 = 2 & \land x_8 = 1 \\
        x_5 = 2 & \land x_6 + 1 = 1 \\ 
        x_5=2\land x_6+1 = 1 & \land 0\leq x_5 \leq 2 \land 0\leq x_6 \leq 2 \\
    \end{aligned}
\label{eq:back1}
\end{equation}

In this approach, we compute the predecessor point by considering both the \texttt{affine} and \texttt{relu} layers together. 
We compute the predecessor point of $x_5=2, x_6=0$ in terms of $x_1$ and $x_2$.
Since $x_6=0$ and $x_6=max(0,x_4)$ then $x_4$ can be anything from its lower bound to $0$ i.e. $-2 \leq x_4 \leq 0$.
Similarly the value of $x_5=2$ and $x_5=max(0,x_3)$ then the value of $x_3$ will be $2$. 
The \sat{} query build as shown in equation~\ref{eq:back2}. 

\begin{equation}
    \begin{aligned}
        x_5 = 2 & \land x_6 = 0 \\
        x_3 = 2 & \land -2\leq x_4 \leq 0 \\ 
        x_1+x_2=2\land -2\leq  x_1+x_2 \leq 0 & \land -1\leq x_1 \leq 1 \land -1\leq x_2 \leq 1 \\
    \end{aligned}
\label{eq:back2}
\end{equation}

When we check the satisfiability of equation~\ref{eq:back2}, it returns \unsat{}. It means the predecessor point of 
$x_5=2,x_6=0$ does not exist, which implies that point $x_5=2, x_6=0$ is introduced by the triangle approximation.
Whenever the solver returns \unsat{}, we also get the \unsatcore{}~\ref{sec:solver}. In this example we get the 
\unsatcore{}=$\{x_1+x_2=2, -2\leq  x_1+x_2 \leq 0\}$, so, we mark the corresponding neurons $x_3$ and $x_4$ as a marked neurons. 
And refine the marked neurons in the refinement procedure. 


\textbf{Refinement} \\
We have two approaches for the refinement {\em Splitting based approach} and {\em MILP-based approach}.
We elaborate here the {\em MILP-based approach} only. The other approach can be seen in algorithm~\ref{algo:verif2}. 
These two approaches refine the marked neurons. We got two different sets of marked neurons in the above analysis. 
The {\em pullback} approach returns $\{x_5, x_6\}$ as the marked neurons, while {\em optimization-based approach}
return $\{x_6\}$ as the marked neurons. In the following analysis, we are taking the marked neurons set as $\{x_5, x_6\}$.
Although the analysis will also work if we take the other set of marked neurons.

In {\em MILP based approach}, we add the exact encoding of the marked neuron ($x_5, x_6$) in addition to the constraints
in equation~\ref{eq:conjunction} and check the satisfiability, now it becomes \unsat{}, hence, the property verified. 
The exact constraint of a $\relu${} neuron is explained in equation~\ref{eq:reluexact}. 

The {\em Splitting based approach} splits the bounds at $0$ of the incoming neurons of the marked neurons. 
The incoming neurons of $x_5$ and $x_6$ are $x_3$ and $x_4$ respectively. 
The affine expression of both the incoming neurons is $x_1+x_2$. 
When we split both $x_3$ and $x_4$ then four cases get generated. The property should get verified in all four cases. 
All the cases are explained as follows: 
% Suppose I split $x_3$ then two cases generated
% $x_3 > 0$ and $x_3 \leq 0$, which implies $x_1+x_2 > 0$ and $x_1+x_2 \leq 0$ respectively. 
% When we split both $x_3$ and $x_4$ then four cases get generated. Out of four cases $x_3 > 0 \land x_4 \leq 0$ and 
% $x_3 \leq 0 \land x_4 > 0$ are infeasible cases, because the corresponding affine expression 
% $x_1+x_2 > 0 \land x_1 + x_2 \leq 0$ is infeasible. We analyze the remaining tow cases as follow: 
\begin{itemize}
    \item $x_3 > 0$ and $x_4 > 0$, since neurons are in active phase, so, upper and lower expressions for $x_5$ 
    becomes $x_3$, similarly the upper and lower expression for $x_6$ becomes $x_4$. 
    The expression for $x_7$ and $x_8$ remains the same. The upper bound of $x_7 - x_8$ computed in equation~\ref{eq:split1} is $-1$. 
    The property got verified.   
    \item $x_3 \leq 0$ and $x_4 \leq 0$, since both neurons are in passive phase, so, the upper and lower expressions 
        for both the neurons become $0$. The upper and lower expressions of the other neurons remain same. 
        The upper bound of $x_7 - x_8$ is computed $-1$ in equation~\ref{eq:split2}. The property got verified here too. 
    \item $x_3 \leq 0$ and $x_4 > 0$, is infeasible case because $x_1+x_2 \leq 0 \land x_1 + x_2 > 0$ is infeasible. 
    \item $x_3 > 0$ and $x_4 \leq 0$, is infeasible case because $x_1+x_2 > 0 \land x_1 + x_2 \leq 0$ is infeasible. 
\end{itemize}

\begin{equation}
    \begin{aligned}
        x_7 - x_8 \leq & x_5 - x_6 - 1 \\
       \leq & x_3 - x_4 -1 \\
       \leq & x_1 + x_2 -(x_1+x_2) -1 \\
       \leq & -1 
    \end{aligned}
    \label{eq:split1}
\end{equation}

\begin{equation}
    \begin{aligned}
      x_7 - x_8 \leq & x_5 - x_6 - 1 \\
        \leq & 0 - 0 - 1 \\
        \leq & -1
    \end{aligned}
    \label{eq:split2}
\end{equation}