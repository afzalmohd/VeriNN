We define the notions and definitions in this section. 
These notions and definitions are used in the later part of the paper. 
The network is defined in the definition \ref*{def:net}. 

% l_{in}, l_{out}
\begin{df}
    \label{def:net}
    A neural network $N = (Neurons, Layers, Edges, W, B, LayerType)$ is a tuple, where
    \begin{itemize}
        \item $Neurons$ is the set of neurons in $N$,
        \item $Layers = \{l_0,...,l_k\}$ is an indexed partition of $Neurons$,
        % \item $l_i \in Layers$ represents the $i^{th}$ layer in network $N$. 
        % \item $|l_i|$ represents the number of neurons in layer $l_i$. 
        % \item 
        \item $ Edges \subseteq \Union_{i=1}^{k} l_{i-1} \times l_{i}$
        \item $W : Edges \mapsto \reals$
        \item $B : Neurons \mapsto \reals$
        \item $LayerType : Layers \mapsto \{\mathtt{Affine}, \mathtt{Relu}\}$
    \end{itemize}
\end{df}

A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents the number of layers.
We call $l_0$ and $l_k$ the {\em input} and {\em output layers} respectively, and all the other layers 
as {\em hidden layers}. We assume a separate layer for activation function. Though there are different kinds of
activation, but we focus only on \relu{}, hence each layer can either be an affine layer or an \relu{} layer.
There is a one to one mapping between an affine layer $l_{i-1}$ and a \relu{} layer $l_i$. 
% If a layer $l_i$ is \relu{} layer, then the input neuron of $n_{ij}$ will only be the neuron $n_{(i-1)j}$. 
Let us say a vector $\boldsymbol{val_i} = [val_{i0}, val_{i1}, ... val_{im}]$ represents the values of each neuron 
in the layer $l_i$, where $m$ is the number of nodes in the same layer.
The value of $\boldsymbol{val_i}$ computed by the weighted sum of the previous layer's values($W_i * V_{i-1} + B_i$)
if $l_i$ is affine layer, otherwise $\boldsymbol{val_i}$ computed by the \relu{} function. 
A function $y = max(0,x)$ is a \relu{} function which takes an arguments $x$ as input and return the
same value $x$ as output if $x$ is non-negative otherwise return the value 0. 

A neural network can be visualize as a function $N$ which takes an input of $m$ dimensions and gives an 
output of $n$ dimensions. $N$ can be represented as a composition of functions $f_k o f_{l-1} ... o f_1$,
where each function $f_i$ represents either the linear combinations of previous layer's
output or an activation function. Let us say $n_{ij} \in N.Neurons$ represents 
the $i^{th}$ neuron of layer $l_j$, and $x_{ij}$ is a real variable for $n_{ij}$. 

% For any vector $\boldsymbol{v}$, $v_i$ represents it's $i^{th}$ value.  

% Let $\reals$ be the set of real numbers.
% Let $x_{\alpha}$ are unbounded set of real variables, where
% $\alpha$ is arbitrary index for variables.

\begin{df}
    \label{def:linexpr}
    $Linexpr = \{ w_0 + \sum_{i} w_i x_i | w_i \in \reals \land x_i \text{ is a real variable} \}$.
\end{df}
  
\begin{df}
    \label{def:linconstr}
    $Linconstr = \{expr \text{ op } 0 | expr \in LinExpr \land op \in \{\leq, = \}\}$
\end{df}






\begin{df}
  A matrix $W_i \in \reals^{|l_i|\times|l_{i-1}|}$ represents the weight matrix for layer $l_i$, where  
    $$
    W_i[t_1, t_2] = 
    \begin{cases}
      W(e) & e=(n_{(i-1)t_2}, n_{it_1}) \in Edges,\\
      0 & \text{otherwise.}\\
    \end{cases}
    $$
\end{df}

\begin{df}
    A matrix $B_i \in \reals^{|l_i|\times 1}$ represents the bias matrix for layer $l_i$. The entry $B_i[t,0] = B(n_{it})$, where $n_{it} \in Neurons$. 
\end{df}



\subsection{DeepPoly}
\label{sec:deeppoly}
We develop our abstract refinement approaches on top of \texttt{DeepPoly}
abstraction~\cite{deeppoly}, which is an abstraction-based method.
The abstraction uses the combination of
well understood polyhedra~\cite{polyhedra} and box~\cite{boxd} abstract domain.
The abstraction maintains
upper and lower linear expressions
as well as
upper and lower bounds for each neurons.
The variables appearing in upper and lower expressions are only from
the predecessor layer.\todo{Add some intuition}
Formally, we define the abstraction as follows. 
% Globally \texttt{DeepPoly} forms a polyhedron.
% Experimentally, it has better precision in comparison to Box~\cite{} and Zonotope~\cite{}.
% Deeppoly has the abstract transformer of various types of layers and activation functions.

\begin{df}
    For a neuron $x \in N.neurons$,
    an abstract constraint $A(x) = (lb,ub, lexpr, uexpr)$ is a tuple, where
    $lb \in \reals$ is lower bound on the value of $x$,
    $ub \in \reals$ is the upper bound on the value of  $x$,
    $lexpr \in LinExpr$ is the expression for the lower bound, and
    $uexpr \in LinExpr$ is the expression for the upper bound.
\end{df}

% The abstract constraint $A$ is generated by the tool deeppoly~\cite{} as explained in subsection~\ref*{sec:deeppoly}. 

In \texttt{DeepPoly}, we compute the abstraction $A$ as follow:
\begin{itemize}
\item For an affine neuron $x_{ij}$, we set 
  $A(x_{ij}).lexpr := A(x_{ij}).uexpr := \sum_{t=1}^{|l_{i-1}|} W[j,t]*x_{(i-1)t} + B_i[j,0]$.
  They compute the value of $A(x_{ij}).lb$ and $A(x_{ij}).ub$ by back substituting
  the variables in $A(x_{ij}).lexpr$ and $A(x_{ij}).uexpr$ respectively up to input layer.  
\item For a relu neuron $x_{ij} = max(0,x_{(i-1)j})$, we consider three cases:
            \begin{enumerate}
                \item If $A(x_{(i-1)j}).lb \geq 0$ then relu is in active phase and $A(x_{ij}).lexpr = A(x_{ij}).uexpr = x_{(i-1)j}$,
                        and $A(x_{ij}).lb = A(x_{(i-1)j}).lb$ and $A(x_{ij}).ub = A(x_{(i-1)j}).ub$
                \item If $A(x_{(i-1)j}).ub \leq 0$ then relu is in passive phase and $A(x_{ij}).lexpr = A(x_{ij}).uexpr = 0$, 
                        and $A(x_{ij}).lb = A(x_{ij}).ub = 0$
                \item  If $A(x_{(i-1)j}).lb < 0$ and $A(x_{(i-1)j}).ub > 0$ then the behavior of relu is uncertain, and authors
                        do over-approximation. $A(x_{ij}).uexpr = u(x_{(i-1)j} - l) / (u - l)$, 
                        where $u = A(x_{(i-1)j}).ub \text{ and } l = A(x_{(i-1)j}).lb$.
                        And $A(x_{ij}).lexpr = \lambda . x_{(i-1)j}$, where $\lambda \in \{0,1\}$. 
                        They are choosing the value of $\lambda$ dynamically. They compute the value of $A(x_{ij}).lb$ and $A(x_{ij}).ub$ 
                        by back substituting the variables in $A(x_{ij}).lexpr$ and $A(x_{ij}).uexpr$ respectively up to input layer. 
            \end{enumerate} 
\end{itemize}

The constraints for an affine neuron are exact because it is just an affine transformation of input neurons. 
The constraints for a relu neuron is also exact if relu is either in active or passive phase. 
The constrains for relu is over-approximated if the behavior of relu is uncertain, although the exact 
constraints for this case exist, but with the cost of efficiency. 
The equation ~\ref{eq:reluexact} shows the exact constraints
for a relu neurons when its behavior is uncertain. Let us say the relu neuron is $x_{ij} = max(0,x_{(i-1)j})$. 

\begin{align}
    \label{eq:reluexact}
    \begin{split}
        x_{ij} &\leq x_{(i-1)j} - l*(1-a) \\
        x_{ij} &\geq x_{(i-1)j} \\
        x_{ij} &\leq u*a \\
        x_{ij} &\geq 0 \\
        a &\in \{0,1\} \\ 
        \text{where }l = A(x_{(i-1)j}).lb &\text{ and }u = A(x_{(i-1)j}).ub \\
    \end{split}
\end{align}


\subsection{Solver}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
