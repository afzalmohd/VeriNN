In this section, We present the notions and the relevant definitions.
% These notions and definitions are used in the later part of the paper. 
We define a neural network as follows. %\ref{def:net}. 

% l_{in}, l_{out}
\begin{df}
  \label{def:net}
  \todo{Relu layer also have W and B?}
    A neural network $N = (Neurons, Layers, Edges, W, B, Type)$ is a tuple, where
    \begin{itemize}
        \item $Neurons$ is the set of neurons in $N$,
        \item $Layers = \{l_0,...,l_k\}$ is an indexed partition of $Neurons$,
        % \item $l_i \in Layers$ represents the $i^{th}$ layer in network $N$. 
        % \item $|l_i|$ represents the number of neurons in layer $l_i$. 
        % \item 
        \item $ Edges \subseteq \Union_{i=1}^{k} l_{i-1} \times l_{i}$
        \item $W : Edges \mapsto \reals$
        \item $B : Neurons \mapsto \reals$
        \item $Type : Layers \mapsto \{\mathtt{Affine}, \mathtt{Relu}\}$
    \end{itemize}
\end{df}

A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents
the number of layers.
We assume $l_i$ are also indexed.
% 
Let $n_{ij}$ be the $j$th neuron of layer $l_i$.
%
We call $l_0$ and $l_k$ the {\em input} and {\em output layers} respectively, and all
the other layers as {\em hidden layers}.
%
In our presentation, we assume separate layers for the activation functions.
%
Though there are different kinds of
activation, we focus only on $\relu${}, hence each layer can either be  $\affine${} or $\relu${} layer.
%
Without loss of generality, we assume that the output layer is an affine layer 
since we can always append an identity affine layer in the neural network, also
the layers $l_1, l_3, l_5, ..., l_k$ 
are the affine layers and $l_2, l_4, l_6, ..., l_{k-1}$ are the relu layers.  
%
If $Type_i = \relu$, then $|l_{i-1}| = |l_{i}|$.
A matrix $W_i \in \reals^{|l_i|\times|l_{i-1}|}$ represents the weight matrix for layer $l_i$, where
\todo{Cleaner notation for $W_i$}
$$
W_i[t_1, t_2] = 
\begin{cases}
  W(e) & e=(n_{(i-1)t_2}, n_{it_1}) \in Edges,\\
  0 & \text{otherwise.}\\
\end{cases}
$$
A matrix $B_i \in \reals^{|l_i|\times 1}$ represents the bias matrix for layer $l_i$. The entry $B_i[t,0] = B(n_{it})$, where $n_{it} \in Neurons$. 

% There is a one-to-one mapping between an affine layer $l_{i-1}$ and a $\relu${} layer $l_i$. 
% If a layer $l_i$ is $\relu${} layer, then the input neuron of $n_{ij}$ will only be the neuron $n_{(i-1)j}$. 


Let us say a vector $\boldsymbol{val_i} = [val_{i1}, val_{i2}, ... val_{i|l_i|}]$ represents the values
of each neuron 
in the layer $l_i$.
Let $f_i$ be a function that computes the output of a layer for layer $i$,
i.e., $val_i = f_i(val_{i-1})$.
For each type the layer the functions are defined as follows.
If $Type_i = \affine$, then $f_{i}(val_{i-1}) = W_i * val_{i-1} + B_i$.
If $Type_i = \relu$, then $f_{i}(val_{i-1})_j =  max(val_{{(i-1)}j},0)$.
%
A neural network is a function $N$ which takes an input of $|l_0|$ dimensions and gives an 
output of $|l_k|$ dimensions. $N$ is a a composition of functions $f_k o ... o f_1$.
For an input $v$, let $val^v_{i}$ denote $f_i o ... o f_1(v)$.

Let us define 
$Linexpr = \{ w_0 + \sum_{i} w_i x_i | w_i \in \reals \text{ and } x_i \text{ is a real variable} \}$
and
$Linconstr = \{expr \text{ op } 0 | expr \in LinExpr \land op \in \{\leq, = \}\}$.
A {\em predicate} is a Boolean combination of $Linconstr$.
We use real variable $x_{ij}$ to represent values of $n_{ij}$ in the predicates.

Let $P$ and $Q$ be a predicate over input and output layers respectively.
A {\em verification query} is a triple $\langle N, P, Q \rangle$.
We need to prove that for each input $\boldsymbol{v}$,
if $\boldsymbol{v} \models P$, $N(\boldsymbol{v}) \models Q$.
We assume $P$ is of the form
% Let us say $P$ is a predicate on input layer's variables
$\Land_{i=1}^{|l_0|}lb_{0i} \leq x_{0i} \leq ub_{0i}$, where $lb_{0i}$ and $ub_{0i}$ are lower and upper bounds respectively for a neuron $n_{0i}$.


% The value of $\boldsymbol{val_i}$ is computed by the weighted sum of the
% previous layer's values($W_i * V_{i-1} + B_i$)
% if $l_i$ is an affine layer, otherwise $\boldsymbol{val_i}$ is computed by the $\relu${} function. 
% A function $y = max(0,x)$ is a $\relu${} function that takes an argument $x$ as input and returns the
% same value $x$ as output if $x$ is non-negative otherwise return the value 0. 


% where each function $f_i$ represents either the linear combinations of the previous layer's
% output or an activation function.

% Let us say $n_{ij} \in N.Neurons$ represent 
% the $i^{th}$ neuron of layer $l_j$,

% Let us say $val_i^{x}$ represent the point on layer $l_i$ if neural network executes on input $x$. 

% For any vector $\boldsymbol{v}$, $v_i$ represents it's $i^{th}$ value.  

% Let $\reals$ be the set of real numbers.
% Let $x_{\alpha}$ are unbounded set of real variables, where
% $\alpha$ is arbitrary index for variables.

% \begin{df}
%   \label{def:linexpr}
% \end{df}
  
% \begin{df}
%     \label{def:linconstr}
% \end{df}



\subsection{DeepPoly}
\label{sec:deeppoly}

We develop our abstract refinement approaches on top of abstraction based method
\deeppoly{}~\cite{singh2019abstract}, which uses a combination of
well-understood polyhedra~\cite{cousot1978automatic} and box~\cite{cousot1977abstract} abstract domain.
The abstraction maintains upper and lower linear expressions as well as upper and lower bounds for each neuron.
The variables appearing in upper and lower expressions are only from the predecessor layer.
Formally, we define the abstraction as follows. 
% \todo{Add some intuition}
% Globally \texttt{DeepPoly} forms a polyhedron.
% Experimentally, it has better precision in comparison to Box~\cite{} and Zonotope~\cite{}.
% Deeppoly has the abstract transformer of various types of layers and activation functions.


\begin{df}
    For a neuron $n$,
    an abstract constraint $A(n) = (lb,ub, lexpr, uexpr)$ is a tuple, where
    $lb \in \reals$ is lower bound on the value of $n$,
    $ub \in \reals$ is the upper bound on the value of  $n$,
    $lexpr \in LinExpr$ is the expression for the lower bound, and
    $uexpr \in LinExpr$ is the expression for the upper bound.
\end{df}

\deeppoly{} computes the abstraction $A$ as follows.

% The abstract constraint $A$ is generated by the tool deeppoly~\cite{} as explained in subsection~\ref*{sec:deeppoly}. 

\begin{itemize}
\item If $Type_i= \affine$, we set 
  $A(x_{ij}).lexpr := A(x_{ij}).uexpr := \sum_{t=1}^{|l_{i-1}|} W_i[j,t]*x_{(i-1)t} + B_i[j,0]$.
  We compute $A(x_{ij}).lb$ and $A(x_{ij}).ub$ by back substituting
  the variables in $A(x_{ij}).lexpr$ and $A(x_{ij}).uexpr$ respectively up to input layer.
  Since $P$ has lower and upper bounds of the input layer, we can compute
  the bounds for $x_{ij}$.
  
\item If $Type_i= \relu$, we consider the following three cases:
  \begin{enumerate}
  \item If $A(x_{(i-1)j}).lb \geq 0$ then $\relu$ is in active phase and
    $A(x_{ij}).lexpr := A(x_{ij}).uexpr := A(x_{(i-1)j}).lexpr $,
    and $A(x_{ij}).lb := A(x_{(i-1)j}).lb$ and $A(x_{ij}).ub := A(x_{(i-1)j}).ub$
  \item If $A(x_{(i-1)j}).ub \leq 0$ then $\relu$ is in passive phase and
    $A(x_{ij}).lexpr := A(x_{ij}).uexpr := 0$, 
    and $A(x_{ij}).lb := A(x_{ij}).ub := 0$.
  \item  If $A(x_{(i-1)j}).lb < 0$ and $A(x_{(i-1)j}).ub > 0$,
    the behavior of $\relu$ is uncertain, and we need to apply
    over-approximation. We set $A(x_{ij}).uexpr := u(x_{(i-1)j} - l) / (u - l)$, 
    where $u = A(x_{(i-1)j}).ub \text{ and } l = A(x_{(i-1)j}).lb$.
    And $A(x_{ij}).lexpr := \lambda . x_{(i-1)j}$, where $\lambda \in \{0,1\}$. 
    We can choose any value of $\lambda$ in a run.
    We compute $A(x_{ij}).lb$ and $A(x_{ij}).ub$ in the same way as
    in the case of affine neurons.
    % They compute the value of $A(x_{ij}).lb$ and $A(x_{ij}).ub$ 
    % by back substituting the variables in $A(x_{ij}).lexpr$ and
    % $A(x_{ij}).uexpr$ respectively up to the input layer. 
  \end{enumerate} 
\end{itemize}

The constraints for an affine neuron are exact because it is just an affine
transformation of input neurons. 
The constraints for a relu neuron are also exact if the relu is either
in the active or passive phase. 
The constraints for relu are over-approximated if the behavior of relu
is uncertain. Although we may compute exact 
constraints for this case, but the constraints will be arbitrary polyhedron,
which are expensive to compute.
The \deepoly abstraction finds a balance between precision and efficiency.

% Equation ~\ref{eq:reluexact} shows the exact constraints
% for a relu neuron when its behavior is uncertain. Let us say the relu neuron is $y = max(0,x)$. 

% \begin{align}
%     \label{eq:reluexact}
%     \begin{split}
%         y &\leq x - l*(1-a) \\
%         y &\geq x \\
%         y &\leq u*a \\
%         y &\geq 0 \\
%         a &\in \{0,1\} \\ 
%         \text{where }l = A(x).lb &\text{ and }u = A(x).ub \\
%     \end{split}
% \end{align}


\subsection{Solver}
\label{sec:solver}

In our algorithms, we will be needing a optimizer and a solver.
To solve the verification queries,
we are using Gurobi(v9.1)~\cite{gurobioptimizer} to optimize the query or to check the satisfiability. 
The function \texttt{checkSAT} in algorithm \ref{algo:refine1}, \ref{algo:verif1}, and
\ref{algo:verif2} 
takes a Boolean formula as input and returns \texttt{SAT} or \texttt{UNSAT}. 
In the case of \texttt{UNSAT}, it returns the \unsatcore{}. 
The \unsatcore{} is a subset of constraints, which are \texttt{UNSAT}, 
but if a single constraint removed from \unsatcore{} then it becomes \texttt{SAT}.
The function \texttt{maxsoftConstr} in the algorithm \ref{algo:refine2} takes
two arguments as input \texttt{hardConstr} and \texttt{softConstrs}. 
The \texttt{hardConstr} is a Boolean formula of constraints,
and \texttt{softConstrs} is a set of constraints. 
The function \texttt{maxsoftConstr} satisfies the maximum number of constraints
in \texttt{softConstrs} while satisfying the \texttt{hardConstr}. 
The function \texttt{maxsoftConstr} returns \texttt{SAT} with the set of
constraints satisfied in \texttt{softConstr}, or returns
\texttt{UNSAT} if \texttt{hardConstr} fails to satisfy.
We use an optimization query in gurobi to implement \texttt{maxsoftConstr} function.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
