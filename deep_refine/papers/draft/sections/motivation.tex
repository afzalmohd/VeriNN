Consider a neural network in figure~\ref{fig:motivating}, 
which has one input, one hidden, and one output layer. The hidden layer is separated into two layers 
\texttt{Affine} and \texttt{ReLU} layers, so a total of four layers is shown in figure~\ref{fig:motivating}. 
Every layer contains two neurons. The neuron $x_8$ has a bias of $1$ and all the other neurons have a bias of $0$. 
Our goal is to verify for all possible input $x_1,x_2 \in [-1,1]$ the outputs $x_7 \leq x_8$ satisfy. 
Our approach is top on \deeppoly{}~\cite{singh2019abstract}. \deeppoly{} maintains the one the upper and one lower constraint
as well as upper and lower bound for each neuron. For a neuron of the affine layer, the upper and lower constraint is 
the same, which is the weighted sum of the input neurons i.e. $x_3$'s upper and lower constraint is $x_1+x_2$. 
For an activation neuron, the upper and lower expression is computed using triangle approximation~\cite{singh2019abstract}, 
which is briefly explained in section~\ref{sec:deeppoly}. To verify the property $x_7 \leq x_8$, \deeppoly{} creates a 
new expression $x_9 = x_7 - x_8$ and computes the upper bound of $x_9$. The upper bound of $x_9$ should not be greater
than $0$. \deeppoly{} computes the upper bound of $x_9$ by back substituting the expression of $x_7$ and $x_8$ 
from the previous layer, the back substitute further from the previous layer. They keep the back substitute 
up to the input layer. The process of back substitution is shown in equation~\ref{eq:deeppoly}.
After back substitution, the upper bound of $x_9$ is computed as $1$ which is greater than $0$, 
hence, the \deeppoly{} fails to verify the property. 


\begin{equation}
    \begin{aligned}
        x_9 \leq  &  x_7 - x_8 \\
        x_9 \leq  & x_5 - x_6 - 1 \\
        x_9 \leq  & 0.5x_3 + 1 - 1 \\
        x_9 \leq  & 0.5(x_1+x_2) \\
        x_9 \leq  & 1
    \end{aligned}
\label{eq:deeppoly}
\end{equation}

There are two main reasons for the failure of \deeppoly{}. One is it can not maintain the complete correlation 
between the neurons. In this example neurons $x_3$ and $x_4$ has the same expression $x_1+x_2$, so, they always
get the same value. But in the \deeppoly{} analysis process, it may fail to get the same value. Second is the triangle
approximation on relu neurons.
We take the conjunction of upper and lower expressions of each neuron with the negation of the property
as shown in equation~\ref{eq:conjunction},
and use the \milp{} solver to check the satisfiability and tackled the first issue. 
The second issue can be resolved either by splitting the 
bound at zero of the affine node or by using the exact encoding (eq~\ref{eq:reluexact}) instead of triangle approximation.
Both solutions increase the problem size exponentially in terms of relu neurons. 
The problem size will be huge if we repair every neuron of the network. 

\begin{equation}
    \begin{aligned}
        -1 \leq & x_1 \leq 1 \\ 
        -1 \leq & x_2 \leq 1 \\
         x_1 + x_2 \leq & x_3 \leq x_1 + x_2 \\
         x_1 + x_2 \leq & x_4 \leq x_1 + x_2 \\
         0 \leq & x_5 \leq 0.5x_3+1 \\ 
         0 \leq & x_6 \leq 0.5x_4 + 1 \\
         x_5 \leq & x_7 \leq x_5 \\ 
         x_6+1 \leq & x_8 \leq x_6+1 \\
         x_7 & > x_8 \text{ (negation of property)}
    \end{aligned}
\label{eq:conjunction}
\end{equation}

We find the set of important neurons (marked neurons), and repair only marked neurons. 
When \deeppoly{} fails to verify the network, we use an \milp{} solver to check the satisfiability
of equation~\ref{eq:conjunction}. 
If it returns \unsat{} it means the property is verified otherwise we get the satisfying assignment of each variable. 
The satisfying assignment of equation~\ref{eq:conjunction} are in equation~\ref{eq:sat1}
We execute the satisfying assignment $x_1=1,x_2=1$ on the neural network and get the values on each neuron 
as shown in equation~\ref{eq:sat2}. 
The output values $x'_7=2, x'_8=3$ satisfying the property, so, the input $x_1=1, x_2=1$ is an spurious counter-example. 
We have to remove spurious counter example by doing the refinement analysis. 
We are using one approach to find the marked neurons guided by the spurious counter example, 
and one approach to refine (repair) the marked neurons.

% We are using two approaches to find the marked neurons guided by the spurious counter example, 
% and two approaches to refine (repair) the marked neurons.

\begin{equation}
    \begin{aligned}
        x_1=1, x_2=1, x_3=2, x_4=2, x_5=2, x_6=0, x_7=2, x_8=1
    \end{aligned}
\label{eq:sat1}
\end{equation}

\begin{equation}
    \begin{aligned}
        x'_1=1, x'_2=1, x'_3=2, x'_4=2, x'_5=2, x'_6=2, x'_7=2, x'_8=3
    \end{aligned}
\label{eq:sat2}
\end{equation}




\begin{figure}[!ht]
	\centering
	\scalebox{0.8}{\input{fig/nn1.tex}}
	\caption{Hypothetical example of neural network}
	\label{fig:motivating}
\end{figure}


Following is the approach to find the marked neurons.
\\
\textbf{Optimization based approach}\\
The satisfying assignments of equation~\ref{eq:conjunction} are in equation~\ref{eq:sat1}. 
We execute the satisfying assignment $x_1=1,x_2=1$ on the neural network and get values on each neuron as 
shown in equation~\ref{eq:sat2}. We make the point on \texttt{relu} layer as close as possible 
to the point of \texttt{relu} in equation~\ref{eq:sat2}, such that input points $x_1,x_2$ and 
output point $x_7,x_8$ remains the same as in equation~\ref{eq:sat1}. 
Equation~\ref{eq:opt1} shows the constraints which must be satisfied. The first and the last line of equation~\ref{eq:opt1}
ensure that the input and output points remain the same as in equation~\ref{eq:sat1}. 
The constraints $\{x_5=2, x_6=2\}$ are soft constraints, which means maximum number of soft constraints should satisfy. 
The soft constraints define the closeness point on the relu layer of equations~\ref{eq:sat1} and \ref{eq:sat2}. 
Finally, we give to the optimizer the constraints in equation~\ref{eq:opt1} and the soft constraints. 
The constraints of equation~\ref{eq:opt1} must be satisfied because it is the same equation as \ref{eq:conjunction}
except for the fixed input and output points. 
The optimizer returns the constraints from soft constraints which are satisfied with the constraints 
of equation~\ref{eq:opt1}. If optimizer returns all soft constraints($\{x_5=2,x_6=2\}$) it means the output point
$x_7=2, x_8=1$ can be reached from $x_5=2,x_6=2$. But optimizer returns the soft constraint $\{x_5=2\}$, it mean 
$x_6$ is a neurons which is contributing to the output point $x_7=2, x_8=1$. We mark the neuron $x_6$ as 
marked neurons.  

\begin{equation}
    \begin{aligned}
        x_1 = 1 & \land x_2 = 1 \\
        x_3 = x_1 + x_2 & \land x_4 = x_1 + x_2 \\
        0 \leq x_5 = 0.5x_3 + 1 & \land 0\leq x_6 \leq 0.5x_4 + 1 \\ 
        x_7 = x_5 & \land x_8 = x_6 + 1 \\
        x_7 = 2 & \land x_8 = 1
    \end{aligned}
\label{eq:opt1}
\end{equation}
\\
\textbf{Refinement} \\
We have an approach for the refinement named as {\em MILP-based approach}.
We got $x_6$ as the marked neuron in the above analysis. 
In {\em MILP based approach}, we add the exact encoding of the marked neuron ($x_5$) in addition to the constraints
in equation~\ref{eq:conjunction} and check the satisfiability, now it becomes \unsat{}, hence, the property verified. 
The exact constraint of a $\relu${} neuron is explained in equation~\ref{eq:reluexact}. 



% \begin{equation}
%     \begin{align}
%         \text{softConstrs} = \{x_5=2, x_6=0\}
%     \end{align}
% \end{equation}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
