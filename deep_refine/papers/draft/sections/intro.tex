% one paragraph per point
% about 2 pages 



% Current state

% Zoom into deeppoly

% your technique (2 para )


% Experiments:
% tools (why those tools), benchmark, results

% Layout of the rest of the paper


% Why this problem?
Neural networks are being increasingly used in safety-critical systems such as autonomous vehicles, medical diagnosis, and speech recognition~\cite{bojarski2016end,amato2013artificial,hinton2012deep}. It is important not only that such systems behave correctly in theory but also that they are robust in practice. Unfortunately, it is often the case (see e.g., Goodfellow \cite{goodfellow2014explaining}) that a slight change/perturbation in the input can often fool the neural networks into an error. Such errors can be hard to find/analyze/debug as these neural networks contain hundreds of thousands of non-linear nodes.

% What is the problem?
To address this problem, an entire line of research has emerged focussed on automatally proving (or disproving) the robustness of such networks. Since automatic verification of neural networks is an NP-hard problem~\cite{?}\todo{whats the ref?}, researchers use approximations in their methods. Classically, we may divide the methods into two classes, namely complete and incomplete. The methods~\cite{lomuscio2017approach,fischetti2018deep,dutta2018output,cheng2017maximum,katz2017reluplex,katz2019marabou,ehlers2017formal,huang2017safety,wang2021beta,xu2020fast,zhang2022general} are complete methods, i.e., they explore the exact state space. Since complete methods explore exact state space, they suffer from scalability issues on large-scale networks. On the other hand, abstraction based methods e.g., \cite{dvijotham2018dual}, \cite{gehr2018ai2}, \cite{singh2018fast},  \cite{singh2018boosting}, \cite{weng2018towards}, \cite{wong2018provable}, \cite{zhang2018efficient}, \cite{zhang2018efficient} are sound and incomplete, because they over-approximate the state space, but they scale extremely well to large benchmarks. A representative method \deeppoly{}~\cite{singh2019abstract} maintains and propagates upper and lower bound constraints using the so-called triangle approximation (also see Section~\ref{sec:deeppoly}). This is also sometimes called bound-propagation. %a single upper and a single lower linear constraint as well as lower and upper bounds for each neuron in the network. For an affine neuron, the upper and lower constraints are the same as an affine expression, which is a weighted sum of the input neurons. For a $\relu${} neuron, upper and lower constraints are constructed by the so-called triangle approximation.
Unsurprisingly, \deeppoly{} and other abstraction based methods suffer from imprecision. Hence, the methods \cite{wang2018formal,wang2018efficient,elboher2020abstraction,yang2021improving,lin2020art} refine the over-approximated state space to achieve completeness. In \cite{wang2018formal,wang2018efficient,lin2020art} the authors eliminate the spurious information (i.e., imprecision introduced by abstraction) by bisecting the input space on the guided dimension. In~\cite{yang2021improving}, which also works on top of \deeppoly{}~\cite{singh2019abstract}, the authors remove the spurious region by  conjuncting each neuron's constraints with the negation of the robustness property and using an MILP optimizer Gurobi~\cite{gurobioptimizer} to refine the bounds of neurons. Finally, in Elbohar et.al. \cite{elboher2020abstraction}, four classes of neurons based on their characteristics are defined. At the time of abstraction, they merge each neuron into one of the four classes.  After completing the abstraction process they use the state of the complete verifier to verify  the abstract network, and go to the refinement process in case of failure.   In the refinement, process authors split the merged neurons.  In the worst case, this method may get back to the original network.  Although this work is a cegar-based approach\todo{afzal, this part is unclear still}, it also suffers from scalability issues on large-scale networks. %  The work \cite{elboher2020abstraction} refinement process focuses on the structure of the networks.

In this paper, we consider the framework provided by \deeppoly{} and develop a novel abstraction-refinement technique that is {\em counter-example guided}, in that we use counter-examples generated from imprecisions during abstraction to guide the refinement process. Our main contributions are the following:
\begin{itemize}
\item We introduce a new {\em optimization-based} technique to find the cause of spuriousness. More precisely, we first use a \milp{} solver to obtain an input where the abstraction does not get verified. Then, we check whether the input generates a real counter-example of falsification of the property or if it is spurious, by executing the neural net. If it is a spurious counter-example, we identify the neuron that caused it.  This gives us the marked neuron which we will refine.
\item We adapt the existing refinement framework built on existing ideas from {\em MILP based}-methods and implement this as a CEGAR algorithm on top of \deeppoly{}.
\item We show that our technique outperforms all to-the-best-of-our-knowledge existing refinement strategies based on \deeppoly{}. <some statistics here>
\item We further demonstrate that our implementation is able to verify benchmarks that are beyond the reach of the best state-of-the-art tools, as discussed next.
\end{itemize}
%Thus, whenever \deeppoly{} fails to verify a property, we conjunct the linear constraints generated by it with the negation of the property, and check for satisfiability, by using an \milp{}-based tool.  If the tool return \unsat{} then we report property \texttt{verified}. Otherwise, we go to the refinement process. We have two parts of our refinement approach, one finds the causes of spurious information  and the second part refines the information gets in the first part.

%Given our focus on the cause of spuriousness, let us now explain at a high level what our counter-examples are and how we use them. TOREWRITE: PICTURE AND EXPLANATION HERE.



%\subsection{Related work}
Another extremely successful line of research has been to revisit the branching heuristics for refinement and use ideas from convex optimization instead of linear or mixed integer linear programming. Starting from a slightly different abstraction/bound propagation method CROWN~\cite{crown}, the work in \cite{betacrown} adopts this approach. This is amenable to parallelizing and hence good for GPU implementation~\cite{gpucrown}. Recently using cutting planes to improve further the refinement analysis has resulted in solving more benchmarks, even at the cost of speed~\cite{cutting-planes}. The success of this approach can be seen by the fact that the state-of-the-art tool ABCROWN (which really refers to a collection of algorithms) has won the VNNCOMP in a very competitive field of leading tools for robustness verification. Despite this success, there still many benchmarks that are out of reach of the highly optimized ABCROWN. Surprisingly, we show that 172 benchmarks that cannot be solved by ABCROWN can be solved by our tool. The reason is that our focus is on identifying the source of imprecision, which is in some sense, orthogonal to ABCROWN's focus on techniques to fix the imprecisions. Integrating our counter-example guided approach for imprecision-identification with ABCROWN's convex optimization based refinement strategies would be the next step towards wider coverage and performance. %As a next step, our goal Hence we believe that these ideas are orthogonal. 




%\subsection{Structure of the paper}

We will concretize the above with a motivating example in the next section~\ref{sec:motivation}.  We define the notions and definitions in section~\ref{sec:model}. The section~\ref{sec:algo} contains the 
algorithm procedure of our approach and section~\ref{sec:experiments} contains the experiments. 
We present the related work and future work in sections \ref{sec:related} and \ref{sec:conclusion} respectively. 



% --- do not erase below this line ----

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
