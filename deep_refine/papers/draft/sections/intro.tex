% one paragraph per point
% about 2 pages 



% Current state

% Zoom into deeppoly

% your technique (2 para )


% Experiments:
% tools (why those tools), benchmark, results

% Layout of the rest of the paper


% Why this problem?
In recent years, neural networks are being increasingly used in safety-critical systems such as autonomous vehicles, medical diagnosis, and speech recognition~\cite{bojarski2016end,amato2013artificial,hinton2012deep}. Hence it is important not only that such systems behave correctly in theory but also that they are robust in practice. Unfortunately, it is now well-established (see e.g., Goodfellow \cite{goodfellow2014explaining}) that a slight change/perturbation in the input can often fool the neural networks into an error. Such errors can be hard to find/analyze/debug as these neural networks contain hundreds of thousands of non-linear nodes.

% What is the problem?
To address this problem, an entire line of research has emerged focussed on automatic (robustness) verification of the networks. Since automatic verification of the neural network is an NP-hard problem~\cite{?}, therefore researchers use approximations in their methods. Classically, we may divide the methods into two classes, namely complete and incomplete. The methods~\cite{lomuscio2017approach,fischetti2018deep,dutta2018output,cheng2017maximum,katz2017reluplex,katz2019marabou,ehlers2017formal,huang2017safety,wang2021beta,xu2020fast,zhang2022general} are complete methods, i.e., they explore the exact state space. Since complete methods explore exact state space, they suffer from scalability issues on the large-scale networks. On the other hand, the abstraction based methods \cite{dvijotham2018dual}, \cite{gehr2018ai2}, \cite{singh2018fast},  \cite{singh2018boosting}, \cite{weng2018towards}, \cite{wong2018provable}, \cite{zhang2018efficient}, \cite{zhang2018efficient} are sound and incomplete because they over-approximate the state space, but they scale extremely well to large benchmarks. A representative method \deeppoly{}~\cite{singh2019abstract} maintains and propagates upper and lower bound constraints using the so-called triangle approximation (also see Section~\ref{sec:deeppoly}). This is also sometimes called bound-propagation. %a single upper and a single lower linear constraint as well as lower and upper bounds for each neuron in the network. For an affine neuron, the upper and lower constraints are the same as an affine expression, which is a weighted sum of the input neurons. For a $\relu${} neuron, upper and lower constraints are constructed by the so-called triangle approximation.
Unsurprisingly, \deeppoly{} and all such abstraction based methods suffer from imprecision. Hence, one very prominent line of research has focussed on \emph{refinement strategies}, i.e., the methods \cite{wang2018formal,wang2018efficient,elboher2020abstraction,yang2021improving,lin2020art} refine the over-approximated state space to achieve completeness. In \cite{wang2018formal,wang2018efficient,lin2020art} the authors eliminate the spurious information by bisecting the input space on the guided dimension. The \cite{yang2021improving} works on top of \deeppoly{}~\cite{singh2019abstract}, they remove the spurious region by taking each neuron's constraints with the negation of the property and using the MILP optimizer Gurobi~\cite{gurobioptimizer} to refine the bounds of neurons. Elbohar et.al. \cite{elboher2020abstraction} defined the four classes of neurons based on their characteristics. At the time of abstraction, they merge each neuron into one of the four classes.  After completing the abstraction process they use the state of the complete verifier to verify  the abstract network, and go to the refinement process in case of failure.   In the refinement, process authors split the merged neurons.  In the worst case, this method may get back to the original network.  Although this work is a cegar-based approach\todo{afzal, what are the cex here?}, it also suffers from scalability issues on large-scale networks. %  The work \cite{elboher2020abstraction} refinement process focuses on the structure of the networks.


In this paper, we consider the framework provided by \deeppoly{} and develop a technique to remove the imprecisions by doing a novel refinement, which is \emph{counter-example guided}. Whenever \deeppoly{} fails to verify the property, we collect all the linear constraints generated by it, with the negation of the property, and check for satisfiability, by using an \milp{}-based tool.  If the tool return \unsat{} then we report property \texttt{verified}. Otherwise, we go to the refinement process. We have two parts of our refinement approach, one finds the causes of spurious information  and the second part refines the information gets in the first part.  Our main contribution is a new {\em optimization-based} technique to find the cause of spuriousness. For refinement, we build on existing ideas from {\em MILP based}-method. We implement our algorithms to obtain a prototype tool that outperforms all other refinement strategies based on \deeppoly{}.

Given our focus on cause of spuriousness, let us now explain at a high level what our counter-examples are and how we use them. TOREWRITE: PICTURE AND EXPLANATION HERE.



\subsection{Related work}
Another extremely successful line of research has been to revisit the branching heuristics for refinement and use ideas from convex optimization instead of linear or mixed integer linear programming. Starting from a slightly different abstraction/bound propagation method CROWN~\cite{crown}, the work in \cite{betacrown} adopts this approach. This is amenable to parallelizing and hence good for GPU implementation~\cite{gpucrown}. Recently using cutting planes to improve further the refinement analysis has resulted in solving more benchmarks, even at the cost of speed~\cite{cutting-planes}. The success of this approach can be seen by the fact that the state-of-the-art tool ABCROWN has won the VNNCOMP in a very competitive field of leading tools for robustness verification. Despite this success, there still many benchmarks that are out of reach of the highly optimized ABCROWN. Surprisingly, we show that 172 benchmarks that cannot be solved by ABCROWN can be solved by our tool. Our focus is on identifying the source of imprecision, orthogonal to ABCROWN's focus on techniques to fix the imprecisions. Integrating our counter-example guided approach for imprecision-identification with ABCROWN's convex optimization based refinement strategies would be the next step towards wider performance. %As a next step, our goal Hence we believe that these ideas are orthogonal. 




\subsection{Structure of the paper}

We will concretize the above with a motivating example in the next section~\ref{sec:motivation}.  We define the notions and definitions in section~\ref{sec:model}. The section~\ref{sec:algo} contains the 
algorithm procedure of our approach and section~\ref{sec:experiments} contains the experiments. 
We present the related work and future work in sections \ref{sec:related} and \ref{sec:conclusion} respectively. 



% --- do not erase below this line ----

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
