%\clearpage
A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents the number of layers. 
Layer $l_0$ and $l_k$ represent the input and output layers respectively and all the other layers are hidden layers.
Each layer $l_i$ is a collection of nodes. A node $v_{i,j}$ represents the $j^{th}$ node in the layer $i$. 
Let us say a vector $\overline{V_i} = [v_{i,0}, v_{i,1}, ... v_{i,m}]$ represents the values of each node in the layer $i$, where $m$ is the number of nodes in the same layer. 
Each layer's values are computed using the weighted sum of the previous layer's values($W_i * V_{i-1} + B_i$) followed by an activation function
\relu{}. A function $y = max(0,x)$ is a \relu{} function which takes an arguments $x$ as input and return the same value $x$ as output if 
$x$ is non-negative otherwise return the value 0. 

A neural network is a function $N$ which takes an input of $m$ dimensions and gives an output of $n$ dimensions.
$N$ can be represented as a composition of functions $f_l o f_{l-1} ... o f_1$, where each function $f_i$ represents the linear combinations followed by an activation function. 

Let $\reals$ be the set of real numbers.
Let $x_{\alpha}$ are unbounded set of real variables, where
$\alpha$ is arbitrary index for variables.


\begin{df}
  $Linexpr = \{ w_0 + \sum_{i} w_i x_i | w_i \in \reals \land x_i \text{ is a real variable} \}$.
\end{df}

\begin{df}
  $Linconstr = \{expr \text{ op } 0 | expr \in LinExpr \land op \in \{\leq, ==, \geq\}\}$
\end{df}

\begin{df}
  $N = (Neurons, Edges, Layers, Input, Output )$.
  \begin{itemize}
    \item $N.Neurons$ represents the neurons in neural network $N$.
    \item $N.Layers$ represents the layers in network $N$.
    \item $l_i \in N.Layers$ represents the $i^{th}$ layer in network $N$. 
    \item $l_i.size$ represents the number of neurons in layer $l_i$. 
    \item $n_{ij} \in N.Neurons$ represents the $j^{th}$ neuron of $i^{th}$ layer.
    \item $x_{ij}$ is a real variable for neuron $n_{ij}$
    \item $\boldsymbol{val_{i}}$ is a value vector at $i^{th}$ layer, if neural network execute on some input vector.
    \item $satval_{ij}$ represents the satisfying value of variable $x_{ij}$, after an optimization query.
    \item For any vector $\boldsymbol{v}$, $v_i$ represents it's $i^{th}$ value.  
    \item $W_i, B_i$ represent the weight and bias matrix of layer $l_i$ respectively. 
  \end{itemize}
\end{df}

\begin{df}
  A vector $\boldsymbol{x} \in \reals^m$ is a counter example(cex) if $N(\boldsymbol{x}) \models Q$.  
\end{df}

\begin{df}
  A vector $\boldsymbol{x} \in \reals^m$ is an spurious counter example(spuriousCEX) if it satisfy all the abstract constraints
  with predicates $P$ and $Q$, but $N(\boldsymbol{x}) \nvDash Q$. 
\end{df}


\begin{df}
  For neuron $x \in N.neurons$,
  an abstract constraint $A(x) = (lb,ub, lexpr, uexpr)$ is a tuple, where
  $lb \in \reals$ is lower bound on the value of $x$,
  $ub \in \reals$ is the upper bound on the value of  $x$,
  $lexpr \in LinExpr$ is the expression for the lower bound, and
  $uexpr \in LinExpr$ is the expression for the upper bound.
  % For a neural network $N$, an abstract constraints $A : N.neurons \mapsto \reals \times \reals \times Expr \times Expr $
\end{df}

Let us say $P$ and $Q$ are the predicates on the input and output space respectively. 
The goal is to find an input $\overline{x_0}$, such that the predicates $P(\overline{x_0})$ and $Q(N(\overline{x_0}))$ holds. 
The predicate $Q$ usually is the negation of the desired property. The triple $\langle N, P, Q \rangle$ is our verification query. 

The algorithm~\ref{algo:main} represents the high level flow of our approach. Which is a counter example guided abstract refinement(CEGAR) based approach. 
At the first line in algorithm~\ref{algo:main}, we generate all the abstract constraints by \deeppoly{}. These abstract constrains
are the lower and upper constraints as well as the lower and upper bounds for each neurons in the neural network.
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both the algorithms~\ref{algo:verif1} and \ref{algo:verif2} takes \markednewrons{} as input. The \markednewrons{} 
is a subset of the neurons of the neural network. The \markednewrons{} represents the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replace the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as a spurious counter example, 
otherwise return verified. Algorithm~\ref{algo:verif2} split each neuron of \markednewrons{} into two sub cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then first case is when $x \in [lb,0]$ and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} run for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
run exponential number of times in the the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
verification query verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the spurious counter example. 


\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State $A = deeppoly(N,P,Q)$\Comment{deeppoly generate the abstract constraints}
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or spuriousCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{spuriousCEX is a real cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} spuriousCEX
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \State constr = \{\}
    \For{$i=1$ to $k$}
      \For{$j=0$ to $l_i.size$}
        \If{$n_{i,j} \in $ markedNeurons}
          \State constr.add(exactConstr($n_{i,j}$)) 
        \Else
          \State constr.add($A(n_{i,j}).lexpr \leq x_{i,j} \leq A(n_{i,j}).uexpr$)
        \EndIf
      \EndFor
    \EndFor
    \State $constr = constr ~ \union ~ P ~ \union ~ Q$
    \State isSat = checkSat(constr)
    \If{isSat is True}
      \State \textbf{return} spurious counter example
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \If{not verified by deeppoly}
        \State constr = \{\}
        \For{$i=1$ to $k$}
          \For{$j=0$ to $l_i.size$}
            \State constr.add($A'(n_{i,j}).lexpr \leq x_{i,j} \leq A'(n_{i,j}).uexpr$)
          \EndFor
        \EndFor
        \State $constr = constr ~ \union ~ P ~ \union ~ Q$ 
        \State isSat = checkSat(constr)
        \If{isSat}
          \State \textbf{return} spurious counter example
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}



\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\boldsymbol{x}$ and $\boldsymbol{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models Q$. 
   \For{$j=0$ to $l_k.size$}
    \State $satval_{kj} = \boldsymbol{y}_j$
   \EndFor
   \For{$i=k$ to $1$}
      \If{$l_i$ is affine layer}
        \State layerConstraints = \{\}
        \For{$j=0$ to $l_i.size$}
          \If{$satval_{i+1,j} > 0$}
            \State $constr_{ij}$ = $(A(n_{ij}).lexpr$ == $satval_{i+1,j}$) \Comment{lexpr=rexpr for affine}
          \Else
            \State $constr_{ij}$ = $(A(n_{ij}).lb \leq A(n_{ij}).lexpr \leq 0$)
          \EndIf
          \State layerConstrains.add($constr_{ij}$)
        \EndFor
        % \For{$j=0$ to $l_{i-1}.size$}\Comment{m is size of layer $l_{i-1}$}
        %   \State layerConstrains.add$(A(x_{i,j}, lb))$
        %   \State layerConstrains.add$(A(x_{i,j}, ub))$
        % \EndFor
        \State isSat = checkSat(layerConstrains) \Comment{And of all constraints}
        \If{not isSat}
          \State markedNeurons = \{$n_{i,j}$ | $0 \leq j\leq l_i.size \land constr_{i,j}$ $\in$ unsatCore\}
          \State \textbf{return } markedNeurons
        \EndIf
      \EndIf
   \EndFor
    \State ce = \{$satval_{0i}$ | $i=0$ to $l_{0}.size$ \} \Comment{If pullbacked upto input layer}
    \State \textbf{return} ce
  \end{algorithmic}
  \caption{A pullback approach to get mark neurons or counter example}
  \label{algo:refine1}
\end{algorithm}


\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\boldsymbol{x}$ and $\boldsymbol{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models Q$. 
    \State Let us say $\boldsymbol{val_{i}}$ is the value vector at layer $l_i$, when $\boldsymbol{x}$ executed on network $N$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      \If{$l_i$ is affine layer}
        \State $\boldsymbol{val_i} = W_i * \boldsymbol{val_{i-1}} + B_i$ \Comment{simple matrix muliplication}
      \Else \Comment{Relu layer}
        \State constraints = \{\}
        \For{$t=i$ to $k$}\Comment{Each layer after $l_i$}
          \For{$j=0$ to $l_t.size$}
            \State constrains.add($A(n_{tj}).lexpr \leq x_{tj}\leq A(n_{tj}).uexpr$)
          \EndFor
        \EndFor
        \For{$j=0$ to $l_{i-1}.size$}\Comment{Previous layer}
          \State constrains.add($x_{i-1,j} == val_{i-1,j}$)
        \EndFor
        \For{$j=0$ to $l_k.size$}\Comment{Output layer}
          \State constrains.add($x_{kj} == y_{j}$)
        \EndFor
        softConstraints = \{\}
        \For{$j=0$ to $l_i.size$}\Comment{Current layer}
          \State softConstraints.add($x_{ij} == val_{ij}$)
        \EndFor
        \State maximize the softConstraints with satisfying the constrains. 
        \If{all the softConstraints satisfied}
          \State \textbf{continue}
        \Else
          \State markedNeurons = all neurons of layer $l_i$ which does not satisfy the softConstraints. 
          \State \textbf{return} markedNeurons
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}






%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
