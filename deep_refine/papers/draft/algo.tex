%\clearpage
A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents the number of layers. 
Layer $l_0$ and $l_k$ represent the input and output layers respectively and all the other layers are hidden layers.
Each layer $l_i$ is a collection of nodes. A node $v_{i,j}$ represents the $j^{th}$ node in the layer $i$. 
Let us say a vector $\overline{V_i} = [v_{i,0}, v_{i,1}, ... v_{i,m}]$ represents the values of each node in the layer $i$, where $m$ is the number of nodes in the same layer. 
Each layer's values are computed using the weighted sum of the previous layer's values($W_i * V_{i-1} + B_i$) followed by an activation function
\relu{}. A function $y = max(0,x)$ is a \relu{} function which takes an arguments $x$ as input and return the same value $x$ as output if 
$x$ is non-negative otherwise return the value 0. 

A neural network is a function $N$ which takes an input of $m$ dimensions and gives an output of $n$ dimensions.
$N$ can be represented as a composition of functions $f_l o f_{l-1} ... o f_1$, where each function $f_i$ represents the linear combinations followed by an activation function. 

Let $\reals$ be the set of real numbers.
Let $x_{\alpha}$ are unbounded set of real variables, where
$\alpha$ is arbitrary index for variables.

\begin{df}
  $Linexpr = \{ w_0 + \sum_{i \in IndexSet} w_i x_i | w_i \in \reals \text{ and IndexSet is a finite set of indexes} \}$.
  % Let $w_i \in \reals$.
  % $w_0 + \sum w_i x_i \in Linexpr$.
\end{df}

\begin{df}
  $N = (Neurons, Edges, Layers, Input, Output )$.
  $N.neurons$?
  $n_{ij}$
  $x_{ij}$ One thing names
  $v_{ij}$ One thing values
\end{df}


\begin{df}
  For neuron $x \in N.neurons$,
  an abstract constraint $A(x) = (lb,ub, lexpr, uexpr)$ is a tuple, where
  $lb \in \reals$ is lower bound on the value of $x$,
  $ub \in \reals$ is the upper bound on the value of  $x$,
  $lexpr \in LinExpr$ is the expression for the lower bound, and
  $uexpr \in LinExpr$ is the expression for the upper bound.
  % For a neural network $N$, an abstract constraints $A : N.neurons \mapsto \reals \times \reals \times Expr \times Expr $
\end{df}

Let us say $P$ and $Q$ are the predicates on the input and output space respectively. 
The goal is to find an input $\overline{x_0}$, such that the predicates $P(\overline{x_0})$ and $Q(N(\overline{x_0}))$ holds. 
The predicate $Q$ usually is the negation of the desired property. The triple $\langle N, P, Q \rangle$ is our verification query. 

The algorithm~\ref{algo:main} represents the high level flow of our approach. Which is a counter example guided abstract refinement(CEGAR) based approach. 
At the first line in algorithm~\ref{algo:main}, we generate all the abstract constraints by \deeppoly{}. These abstract constrains
are the lower and upper constraints as well as the lower and upper bounds for each neurons in the neural network.
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both the algorithms~\ref{algo:verif1} and \ref{algo:verif2} takes \markednewrons{} as input. The \markednewrons{} 
is a subset of the neurons of the neural network. The \markednewrons{} represents the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replace the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as a spurious counter example, 
otherwise return verified. Algorithm~\ref{algo:verif2} split each neuron of \markednewrons{} into two sub cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then first case is when $x \in [lb,0]$ and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} run for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
run exponential number of times in the the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
verification query verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the spurious counter example. 


\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State Build abstract constrains and bounds on each neuron using deeppoly.
    \State $A$ is a set of all abstract constraints and bounds. 
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or spuriousCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{spuriousCEX is a real cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} spuriousCEX
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \For{nt in markedNeurons}
      \State $A = (A ~ \union ~ exactConstr(nt)) \setminus abstractConstr(nt)$
    \EndFor
    \State $A = A ~ \union ~ P ~ \union ~ \neg Q$
    \State isSat = checkSat(A)
    \If{isSat is True}
      \State \textbf{return} spurious counter example
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State run deeppoly
      \If{not verified by deeppoly}
        \State A = Set of all abstract constraints generated by deeppoly
        \State $A = A ~ \union ~ P ~ \union ~ \neg Q$ 
        \State isSat = checkSat(A)
        \If{isSat}
          \State \textbf{return} spurious counter example
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}



\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\overrightarrow{x}$ and $\overrightarrow{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
   \State Run neural network on $\overrightarrow{x}$.
   \State \textbf{return} $\overrightarrow{x}$ as counter example if it's output violate the $Q$. 
   \For{currLayer in [outputLayer ... inputLayer]}
      \If{currLayer is affine layer}
        \State layerConstraints = []
        \For{nt in currLayer.neurons}
          \State nt.constr = (nt.affineExpr == nt.satval)
          \State layerConstrains.append(nt.constr)
        \EndFor
        \For{nt in currLayer.prevLayer.neurons}
          \State layerConstrains.append(nt.bounds)
        \EndFor
        \State isSat = checkSat(layerConstrains)
        \If{isSat}
          \State assign sat values to previous layers neurons
        \Else
          \State markedNeurons = \{nt | nt $\in$ currLayer.neurons $\land$ nt.constr $\in$ unsatCore\}
          \State \textbf{return } markedNeurons
        \EndIf
      \Else \Comment{Relu layer}
        \For{nt in currLayer.neurons}
          \If{nt.satval > 0}
            \State prevNt.satval = nt.satval  \Comment{prevNt is input node of nt in prevLayer}
          \Else
            \State prevNt.lb $\leq$ prevNt.satval $\leq$ 0 \Comment{lb,ub are bounds}
          \EndIf
        \EndFor
      \EndIf
   \EndFor
    \State ce = \{nt.satval | nt $\in$ inputLayer.neurons\} \Comment{If pullbacked upto input layer}
    \State \textbf{return} ce
  \end{algorithmic}
  \caption{A pullback approach to get mark neurons or counter example}
  \label{algo:refine1}
\end{algorithm}


\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\overrightarrow{x}$ and $\overrightarrow{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State Run neural network on $\overrightarrow{x}$.
    \State \textbf{return} $\overrightarrow{x}$ as counter example if it's output violate the $Q$. 
    \State Let us say layer.nt.val is the value evaluated on $\overrightarrow{x}$ for each layer and each neuron nt. 
    \For{currLayer in [firstLayer ... outputLayer]} \Comment{inputLayer excluded}
      \If{currLayer is affine layer}
        \State execute currLayer.prevLayer.vals on currLayer \Comment{simple matrix muliplication}
      \Else
        \State constraints = A[currLayer ... outputLayer] \Comment{Abstract constraints from currLayer to outputLayers}
        \State constraints.add(currLayer.prevLayer.vars == currLayer.prevLayer.vals)
        \State constrains.add(outputLayer.vars == outputLayer.vals)
        \State softConstraints = for each nt $\in$ currLayer.neurons (nt.vars == nt.vals)
        \State maximize the number of neurons of layer which satisfy the softConstraints. 
        \If{all neurons of the currLayer satisfy the softConstraints}
          \State \textbf{continue}
        \Else
          \State markedNeurons = all neurons of currLayer which does not satisfy the softConstraints. 
          \State \textbf{return} markedNeurons
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}






%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
