%\clearpage
A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents the number of layers. 
Layer $l_0$ and $l_k$ represent the input and output layers respectively and all the other layers are hidden layers.
Each layer $l_i$ is a collection of nodes. A node $v_{i,j}$ represents the $j^{th}$ node in the layer $i$. 
Let us say a vector $\overline{V_i} = [v_{i,0}, v_{i,1}, ... v_{i,m}]$ represents the values of each node in the layer $i$, where $m$ is the number of nodes in the same layer. 
Each layer's values are computed using the weighted sum of the previous layer's values($W_i * V_{i-1} + B_i$) followed by an activation function
\relu{}. A function $y = max(0,x)$ is a \relu{} function which takes an arguments $x$ as input and return the same value $x$ as output if 
$x$ is non-negative otherwise return the value 0. 

A neural network is a function $N$ which takes an input of $m$ dimensions and gives an output of $n$ dimensions.
$N$ can be represented as a composition of functions $f_l o f_{l-1} ... o f_1$, where each function $f_i$ represents the linear combinations followed by an activation function. 

Let $\reals$ be the set of real numbers.
Let $x_{\alpha}$ are unbounded set of real variables, where
$\alpha$ is arbitrary index for variables.

\begin{df}
  $Linexpr = \{ w_0 + \sum_{i \in IndexSet} w_i x_i | w_i \in \reals \text{ and IndexSet is a finite set of indexes} \}$.
  % Let $w_i \in \reals$.
  % $w_0 + \sum w_i x_i \in Linexpr$.
\end{df}

\begin{df}
  $N = (Neurons, Edges, Layers, Input, Output )$.
  $N.neurons$?
  $n_{ij}$
  $x_{ij}$ One thing names
  $v_{ij}$ One thing values
\end{df}


\begin{df}
  For neuron $x \in N.neurons$,
  an abstract constraint $A(x) = (lb,ub, lexpr, uexpr)$ is a tuple, where
  $lb \in \reals$ is lower bound on the value of $x$,
  $ub \in \reals$ is the upper bound on the value of  $x$,
  $lexpr \in LinExpr$ is the expression for the lower bound, and
  $uexpr \in LinExpr$ is the expression for the upper bound.
  % For a neural network $N$, an abstract constraints $A : N.neurons \mapsto \reals \times \reals \times Expr \times Expr $
\end{df}

Let us say $P$ and $Q$ are the predicates on the input and output space respectively. 
The goal is to find an input $\overline{x_0}$, such that the predicates $P(\overline{x_0})$ and $Q(N(\overline{x_0}))$ holds. 
The predicate $Q$ usually is the negation of the desired property. The triple $\langle N, P, Q \rangle$ is our verification query. 

The algorithm~\ref{algo:main} represents the high level flow of our approach. Which is a counter example guided abstract refinement(CEGAR) based approach. 
At the first line in algorithm~\ref{algo:main}, we generate all the abstract constraints by \deeppoly{}. These abstract constrains
are the lower and upper constraints as well as the lower and upper bounds for each neurons in the neural network.
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both the algorithms~\ref{algo:verif1} and \ref{algo:verif2} takes \markednewrons{} as input. The \markednewrons{} 
is a subset of the neurons of the neural network. The \markednewrons{} represents the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replace the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as a spurious counter example, 
otherwise return verified. Algorithm~\ref{algo:verif2} split each neuron of \markednewrons{} into two sub cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then first case is when $x \in [lb,0]$ and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} run for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
run exponential number of times in the the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
verification query verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the spurious counter example. 


\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State Build abstract constrains and bounds on each neuron using deeppoly.
    \State $A$ is a set of all abstract constraints and bounds. 
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or spuriousCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{spuriousCEX is a real cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} spuriousCEX
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \State constr = \{\}
    \For{$i=1$ to $k$}
      \For{$j=0$ to $l_i.size$}
        \If{$n_{i,j} \in $ markedNeurons}
          \State constr.add(exactConstr($n_{i,j}$)) 
        \Else
          \State constr.add($A(n_{i,j},lexpr) \leq x_{i,j} \leq A(n_{i,j},uexpr)$)
        \EndIf
      \EndFor
    \EndFor
    \State $constr = constr ~ \union ~ P ~ \union ~ \neg Q$
    \State isSat = checkSat(constr)
    \If{isSat is True}
      \State \textbf{return} spurious counter example
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \If{not verified by deeppoly}
        \State constr = \{\}
        \For{$i=1$ to $k$}
          \For{$j=0$ to $l_i.size$}
            \State constr.add($A'(n_{i,j},lexpr) \leq x_{i,j} \leq A'(n_{i,j},uexpr)$)
          \EndFor
        \EndFor
        \State $constr = constr ~ \union ~ P ~ \union ~ \neg Q$ 
        \State isSat = checkSat(constr)
        \If{isSat}
          \State \textbf{return} spurious counter example
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}



\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\overrightarrow{x}$ and $\overrightarrow{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
   \State Run neural network on $\overrightarrow{x}$.
   \State \textbf{return} $\overrightarrow{x}$ as counter example if it's output violate the $Q$. 
   \For{$j=0$ to $l_k.size$}
    \State $var_{k,j} = \overrightarrow{y}_j$
   \EndFor
   \For{$i=k$ to $1$}
      \If{$l_i$ is affine layer}
        \State layerConstraints = \{\}
        \For{$j=0$ to $l_i.size$}
          \State $constr_{i,j}$ = $(A(x_{i,j},lexpr)$ == $var_{i,j}$) \Comment{lexpr=rexpr for affine layer}
          \State layerConstrains.add($constr_{i,j}$)
        \EndFor
        % \For{$j=0$ to $l_{i-1}.size$}\Comment{m is size of layer $l_{i-1}$}
        %   \State layerConstrains.add$(A(x_{i,j}, lb))$
        %   \State layerConstrains.add$(A(x_{i,j}, ub))$
        % \EndFor
        \State isSat = checkSat(layerConstrains) \Comment{And of all constraints}
        \If{isSat}
          \State assign sat values to previous layers neurons
        \Else
          \State markedNeurons = \{$x_{i,j}$ | $0 \leq j\leq l_i.size \land constr_{i,j}$ $\in$ unsatCore\}
          \State \textbf{return } markedNeurons
        \EndIf
      \Else \Comment{Relu layer}
        \For{$j=0$ to $l_i.size$}
          \If{$var_{i,j} > 0$}
            \State $var_{i-1,j} = var_{i,j}$
          \Else
            \State $A(x_{i-1,j},lb) \leq var_{i-1,j} \leq$ 0 \Comment{lb,ub are bounds}
          \EndIf
        \EndFor
      \EndIf
   \EndFor
    \State ce = \{$var_{0,i}$ | $i=0$ to $l_{0}.size$ \} \Comment{If pullbacked upto input layer}
    \State \textbf{return} ce
  \end{algorithmic}
  \caption{A pullback approach to get mark neurons or counter example}
  \label{algo:refine1}
\end{algorithm}


\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\overrightarrow{x}$ and $\overrightarrow{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State Run neural network on $\overrightarrow{x}$.
    \State \textbf{return} $\overrightarrow{x}$ as counter example if it's output violate the $Q$. 
    \State Let us say $\overrightarrow{val_{i}}$ is the value evaluated on $\overrightarrow{x}$ on layer $l_i$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      \If{$l_i$ is affine layer}
        \State $\overrightarrow{val_i} = W_i * \overrightarrow{val_{i-1}} + B_i$ \Comment{simple matrix muliplication}
      \Else
        \State constraints = \{\}
        \For{$t=i$ to $k$}\Comment{Each layer after $l_i$}
          \For{$j=0$ to $l_t.size$}
            \State constrains.add($A(var_{t,j},lexpr) \leq var_{t,j}\leq A(var_{t,j},uexpr)$)
          \EndFor
        \EndFor
        \For{$j=0$ to $l_{i-1}.size$}\Comment{Previous layer}
          \State constrains.add($var_{i-1,j} == val_{i-1,j}$)
        \EndFor
        \For{$j=0$ to $l_k.size$}\Comment{Output layer}
          \State constrains.add($var_{k,j} == val_{k,j}$)
        \EndFor
        softConstraints = \{\}
        \For{$j=0$ to $l_i.size$}\Comment{Current layer}
          \State softConstraints.add($var_{i,j} == val_{i,j}$)
        \EndFor
        \State maximize the softConstraints with satisfying the constrains. 
        \If{all the softConstraints satisfied}
          \State \textbf{continue}
        \Else
          \State markedNeurons = all neurons of layer $l_i$ which does not satisfy the softConstraints. 
          \State \textbf{return} markedNeurons
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}






%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
