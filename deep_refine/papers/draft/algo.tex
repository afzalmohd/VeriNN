%\clearpage
A {\em predicate} is a Boolean combination of $Linconstr$.
Let $P$ and $Q$ be a predicate over input and output layers respectively.
A verification query is a triple $\langle N, P, Q \rangle$.
We need to prove that for each input $\boldsymbol{v}$,
if $\boldsymbol{v} \models P$, $N(\boldsymbol{v}) \models Q$.
We assume $P$ is of the form
% Let us say $P$ is a predicate on input layer's variables
$\Land_{i=1}^{|l_0|}lb_i \leq x_{0i} \leq ub_i$, where $lb_i$ and $ub_i$ are lower and upper bounds for neuron $n_{0i}$ respectively.\todo{Why do we expect this form}
An input vector $\boldsymbol{v} \in \reals^{|l_0|}$ is a counter example(cex) if $N(\boldsymbol{v}) \models \lnot Q$.  

% The goal is to find an input $\boldsymbol{x}$, such that the predicates $P(\boldsymbol{x})$ and 
% $Q(N(\boldsymbol{x}))$ holds. The predicate $Q$ usually is the negation of the desired property. 
% The triple  is our verification query.
%  \todo{How to formalize $P$ and $Q$}


$satval_{ij}$ represents the satisfying value of variable $x_{ij}$, after an optimization query.

% We will use the following convention in writing our algorithms.


% \begin{itemize}
% \item 
% \item 
% % \item $W_i, B_i$ represent the weight and bias matrix of layer $l_i$ respectively. 
% \end{itemize}


% \begin{df}
% \end{df}

Let us say $lc_i = \Land_{j=1}^{|l_i|} A(n_{ij}).lexpr \leq x_{ij} \leq  A(n_{ij}).uexpr$ is a 
conjunction of upper and lower constrains of each neurons of layer $l_i$ with respect to abstract constraint $A$.
The $lexpr$ and $uexpr$ for any neuron of a layer contains variables only from the previous layer's neurons, 
hence $lc_i$ contains the variables from layers $l_{i-1}$ and $l_i$. 

\begin{df}
  A sequence of value vectors $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$ is an 
  {\em abstract execution} of abstract constraint $A$ if 
  $\boldsymbol{v_0} \models lc_0$ and $\boldsymbol{v_{i-1}}, \boldsymbol{v_i} \models lc_i$ for each $i \in [1,k]$.  
 Moreover, an abstract execution $\boldsymbol{v_0,...,v_k}$ is
  an {\em spurious counter example(spuriousCEX)} if $\boldsymbol{v_k} \models \lnot Q$
\end{df}

% \begin{df}
%   An abstract counterexample $\boldsymbol{v_0,...,v_k}$ is a
%   spurious counter example(spuriousCEX) if $\boldsymbol{v_0}$ is not a counterexample.
%   % satisfy all the abstract constraints
%   % with predicates $P$ and $Q$, but $N(\boldsymbol{x}) \nvDash Q$. 
% \end{df}





% Let $P$ be a predicate over variables $x_0$.

% \begin{df}
%   A predicate on input layer is $P = \bigtimes_{i=0}^{m}[lb_i, ub_i]$, where $m$ is the number of neurons 
%   in input layer and $lb_i, ub_i$ are the lower and upper bounds for each neuron $n_{0i}$.   
% \end{df}



The algorithm~\ref{algo:main} represents the high level flow of our approach. Which is a counter example guided abstract refinement(CEGAR) based approach. 
At the first line in algorithm~\ref{algo:main}, we generate all the abstract constraints by \deeppoly{}. These abstract constrains
are the lower and upper constraints as well as the lower and upper bounds for each neurons in the neural network.
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both the algorithms~\ref{algo:verif1} and \ref{algo:verif2} takes \markednewrons{} as input. The \markednewrons{} 
is a subset of the neurons of the neural network. The \markednewrons{} represents the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replace the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as a spurious counter example, 
otherwise return verified. Algorithm~\ref{algo:verif2} split each neuron of \markednewrons{} into two sub cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then first case is when $x \in [lb,0]$ and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} run for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
run exponential number of times in the the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
verification query verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the spurious counter example. 


\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State $A := deeppoly(N,P,Q)$\Comment{deeppoly generate the abstract constraints}
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or spuriousCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{spuriousCEX is a cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} cex
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \State $constr := P \land (\Land_{i=1}^k lc_i)\land \neg Q$
    \State $constr := constr \land (\Land_{x \in markedNeurons} exactConstr(x))$ \Comment{as in eq \ref*{eq:reluexact}}
    % \For{$i=1$ to $k$}
    %   \For{$j=1$ to $|l_i|$}
    %     \If{$n_{i,j} \in $ markedNeurons}
    %       \State $constr := constr \land exactConstr(n_{i,j})$ 
    %     \Else
    %       \State $constr := constr \land A(n_{i,j}).lexpr \leq x_{i,j} \leq A(n_{i,j}).uexpr$
    %     \EndIf
    %   \EndFor
    % \EndFor
    % \State $constr := constr \land P \land \neg Q$
    \State isSat = checkSat(constr)
    \If{isSat}
      \State \textbf{return} spuriousCEX
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \State $constr := P \land (\Land_{i=1}^k lc'_i) \land \neg Q$ \Comment{$lc'$ is with respect to $A'$}
      % \If{not verified by deeppoly}
      %   \State $constr := true$
      %   \For{$i=1$ to $k$}
      %     \For{$j=1$ to $|l_i|$}
      %       \State $constr := constr \land A'(n_{i,j}).lexpr \leq x_{i,j} \leq A'(n_{i,j}).uexpr$
      %     \EndFor
      %   \EndFor
      %   \State $constr := constr  \land P \land \neg Q$ 
        \State isSat = checkSat(constr)
        \If{isSat}
          \State \textbf{return} spuriousCEX
        \EndIf
      %\EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}



\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$, abstract constraints $A$ and $\boldsymbol{x}, \boldsymbol{y} \in spuriousCEX$ satisfying assignments on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models \neg Q$. 
  %  \For{$j=0$ to $|l_k|$}
  %   \State $satval_{kj} = \boldsymbol{y}_j$
  %  \EndFor
   \State $\boldsymbol{satval}_k = \boldsymbol{y}_j$
   \For{$i=k$ to $1$}
      \If{$l_i$ is affine layer}
        \State $layerConstraints := true$
        \For{$j=1$ to $|l_i|$}
          \If{$satval_{i,j} > 0$}
            \State $constr_{ij}$ := $(A(n_{ij}).lexpr$ = $satval_{i,j}$) \Comment{lexpr=uexpr for affine}
          \Else
            \State $constr_{ij}$ := $(A(n_{ij}).lb \leq A(n_{ij}).lexpr \leq 0$)
          \EndIf
          \State $layerConstrains := layerConstrains \land constr_{ij}$
        \EndFor
        \State isSat = checkSat(layerConstrains)
        \If{not isSat}
          \State markedNeurons = \{$n_{i,j}$ | $1 \leq j\leq |l_i| \land constr_{i,j}$ $\in$ unsatCore\}
          \State \textbf{return } markedNeurons
        \EndIf
      \Else \Comment{Relu layer}
        \State $\boldsymbol{satval}_{i-1} := \boldsymbol{satval}_i$
      \EndIf
   \EndFor
    %\State $\boldsymbol{cex}$ = $\boldsymbol{satval_0}$ \Comment{If pullbacked upto input layer}
    \State \textbf{return} $\boldsymbol{satval}_0$ \Comment{cex if pullbacked till input layer}
  \end{algorithmic}
  \caption{A pullback approach to get mark neurons or counter example}
  \label{algo:refine1}
\end{algorithm}


\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\boldsymbol{x}$ and $\boldsymbol{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models Q$. 
    \State Let us say $\boldsymbol{val_{i}}$ is the value vector at layer $l_i$, when $\boldsymbol{x}$ executed on network $N$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      % \If{$l_i$ is affine layer}
      %   \State $\boldsymbol{val_i} = W_i * \boldsymbol{val_{i-1}} + B_i$ \Comment{simple matrix muliplication}
      % \Else \Comment{Relu layer}
      \If{$l_i$ is \relu{} layer}
        \State $constrs := \Land_{t=i}^k lc_t$
        % \For{$t=i$ to $k$}\Comment{Each layer after $l_i$}
        %   \For{$j=1$ to $|l_t|$}
        %     \State $constr := constr \land (A(n_{tj}).lexpr \leq x_{tj}\leq A(n_{tj}).uexpr)$
        %   \EndFor
        % \EndFor
        \State $constr := constr \land \Land_{j=1}^{|l_{i-1}|} (x_{(i-1)j} = val_{(i-1)j})$
        % \For{$j=1$ to $|l_{i-1}|$}\Comment{Previous layer}
        %   \State $constr := constr \land (x_{(i-1)j} = val_{(i-1)j})$
        % \EndFor
        \State $constr := constr \land \Land_{j=1}^{|l_k|} (x_{kj} = y_{j})$
        % \For{$j=1$ to $|l_k|$}\Comment{Output layer}
        %   \State $constr := constr \land (x_{kj} = y_{j})$
        % \EndFor
        \State $softConstraints := \cup_{j=1}^{|l_j|} (x_{ij} = val_{ij})$
        % \For{$j=1$ to $|l_i|$}\Comment{Current layer}
        %   \State softConstraints.add($x_{ij} = val_{ij}$)
        % \EndFor
        \State maximize the $softConstraints$ with satisfying the $constr$. 
        \State $markedNeurons := \{n_{ij} | 1 \leq j \leq |l_i| \land x_{ij} \neq val_{ij}\}$ 
        \If{$markedNeurons$ is empty}
          \State \textbf{continue}
        \Else
          \State \textbf{return} markedNeurons
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}






%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
