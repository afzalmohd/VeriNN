%\clearpage


We will use the following convention in writing our algorithms.
\begin{itemize}
\item $x_{ij}$ is a real variable for neuron $n_{ij}$
\item $\boldsymbol{val_{i}}$ is a value vector at $i^{th}$ layer, if neural network execute on some input vector. And $val_{ij}$ represents the value of $n_{ij}$. 
\item $satval_{ij}$ represents the satisfying value of variable $x_{ij}$, after an optimization query.
% \item $W_i, B_i$ represent the weight and bias matrix of layer $l_i$ respectively. 
\end{itemize}


\begin{df}
  A vector $\boldsymbol{x} \in \reals^m$ is a counter example(cex) if $N(\boldsymbol{x}) \models Q$.  
\end{df}

\begin{df}
  A vector $\boldsymbol{x} \in \reals^m$ is an spurious counter example(spuriousCEX) if it satisfy all the abstract constraints
  with predicates $P$ and $Q$, but $N(\boldsymbol{x}) \nvDash Q$. 
\end{df}


\begin{df}
  For neuron $x \in N.neurons$,
  an abstract constraint $A(x) = (lb,ub, lexpr, uexpr)$ is a tuple, where
  $lb \in \reals$ is lower bound on the value of $x$,
  $ub \in \reals$ is the upper bound on the value of  $x$,
  $lexpr \in LinExpr$ is the expression for the lower bound, and
  $uexpr \in LinExpr$ is the expression for the upper bound.
  % For a neural network $N$, an abstract constraints $A : N.neurons \mapsto \reals \times \reals \times Expr \times Expr $
\end{df}

Let us say $P$ and $Q$ are the predicates on the input and output space respectively. 
The goal is to find an input $\boldsymbol{x}$, such that the predicates $P(\boldsymbol{x})$ and 
$Q(N(\boldsymbol{x}))$ holds. The predicate $Q$ usually is the negation of the desired property. 
The triple $\langle N, P, Q \rangle$ is our verification query. \todo{How to formalize $P$ and $Q$}


The algorithm~\ref{algo:main} represents the high level flow of our approach. Which is a counter example guided abstract refinement(CEGAR) based approach. 
At the first line in algorithm~\ref{algo:main}, we generate all the abstract constraints by \deeppoly{}. These abstract constrains
are the lower and upper constraints as well as the lower and upper bounds for each neurons in the neural network.
The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
Both the algorithms~\ref{algo:verif1} and \ref{algo:verif2} takes \markednewrons{} as input. The \markednewrons{} 
is a subset of the neurons of the neural network. The \markednewrons{} represents the set of the culprit neurons. 
Algorithm~\ref{algo:verif1} replace the abstract constraints of \markednewrons{} by exact constraints and check for the 
property by \milp{} solver. If \milp{} solver return \sat{} then we return satisfying assignments as a spurious counter example, 
otherwise return verified. Algorithm~\ref{algo:verif2} split each neuron of \markednewrons{} into two sub cases. Suppose the 
neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then first case is when $x \in [lb,0]$ and the second case is when $x \in [0,ub]$.   
After splitting neurons into two cases \deeppoly{} run for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
run exponential number of times in the the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
verification query verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the spurious counter example. 


\begin{algorithm}[t]
  \textbf{Input: } A verification problem $\langle N,P,Q \rangle$ \\
  \textbf{Output: } UNSAT or SAT
  \begin{algorithmic}[1]
    \State $A = deeppoly(N,P,Q)$\Comment{deeppoly generate the abstract constraints}
    \State markedNeurons = \{\}
    \While{True}
      \State isVerified or spuriousCEX = isVerified($\langle N,P,Q \rangle$ , A, markedNeurons)
      \If{isVerified is True}
        \State \textbf{return} UNSAT
      \Else
        \If{spuriousCEX is a real cex of $\langle N,P,Q \rangle$}
          \State \textbf{return} spuriousCEX
        \Else
          \State marked, cex = getMarkedNeurons($\langle N,P,Q \rangle$ , A, markedNeurons)
          \If{cex not None}
            \State \textbf{return} cex
          \EndIf
          \State markedNeurons = markedNeurons $\union$ marked
        \EndIf
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{A CEGAR based approach of neural network verification}
  \label{algo:main}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons $\subseteq ~ N.neurons$ \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \State constr = \{\}
    \For{$i=1$ to $k$}
      \For{$j=0$ to $|l_i|$}
        \If{$n_{i,j} \in $ markedNeurons}
          \State constr.add(exactConstr($n_{i,j}$)) 
        \Else
          \State constr.add($A(n_{i,j}).lexpr \leq x_{i,j} \leq A(n_{i,j}).uexpr$)
        \EndIf
      \EndFor
    \EndFor
    \State $constr = constr ~ \union ~ P ~ \union ~ Q$
    \State isSat = checkSat(constr)
    \If{isSat is True}
      \State \textbf{return} spurious counter example
    \Else
      \State \textbf{return} verified
    \EndIf
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif1}
\end{algorithm}

\begin{algorithm}[t]
  \textbf{Name: } isVerified2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
  \textbf{Output: } verified or spurious counter example. 
  \begin{algorithmic}[1]
    \For{all combination in $2^{markedNeurons}$}
      \State $A'$ = run deeppoly
      \If{not verified by deeppoly}
        \State constr = \{\}
        \For{$i=1$ to $k$}
          \For{$j=0$ to $|l_i|$}
            \State constr.add($A'(n_{i,j}).lexpr \leq x_{i,j} \leq A'(n_{i,j}).uexpr$)
          \EndFor
        \EndFor
        \State $constr = constr ~ \union ~ P ~ \union ~ Q$ 
        \State isSat = checkSat(constr)
        \If{isSat}
          \State \textbf{return} spurious counter example
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} verified
  \end{algorithmic}
  \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
  \label{algo:verif2}
\end{algorithm}



\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons1 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\boldsymbol{x}$ and $\boldsymbol{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models Q$. 
   \For{$j=0$ to $|l_k|$}
    \State $satval_{kj} = \boldsymbol{y}_j$
   \EndFor
   \For{$i=k$ to $1$}
      \If{$l_i$ is affine layer}
        \State layerConstraints = \{\}
        \For{$j=0$ to $|l_i|$}
          \If{$satval_{i+1,j} > 0$}
            \State $constr_{ij}$ = $(A(n_{ij}).lexpr$ == $satval_{i+1,j}$) \Comment{lexpr=rexpr for affine}
          \Else
            \State $constr_{ij}$ = $(A(n_{ij}).lb \leq A(n_{ij}).lexpr \leq 0$)
          \EndIf
          \State layerConstrains.add($constr_{ij}$)
        \EndFor
        % \For{$j=0$ to $l_{i-1}.size$}\Comment{m is size of layer $l_{i-1}$}
        %   \State layerConstrains.add$(A(x_{i,j}, lb))$
        %   \State layerConstrains.add$(A(x_{i,j}, ub))$
        % \EndFor
        \State isSat = checkSat(layerConstrains) \Comment{And of all constraints}
        \If{not isSat}
          \State markedNeurons = \{$n_{i,j}$ | $0 \leq j\leq |l_i| \land constr_{i,j}$ $\in$ unsatCore\}
          \State \textbf{return } markedNeurons
        \EndIf
      \EndIf
   \EndFor
    \State ce = \{$satval_{0i}$ | $i=0$ to $|l_0|$ \} \Comment{If pullbacked upto input layer}
    \State \textbf{return} ce
  \end{algorithmic}
  \caption{A pullback approach to get mark neurons or counter example}
  \label{algo:refine1}
\end{algorithm}


\begin{algorithm}[t]
  \textbf{Name: } getMarkedNeurons2 \\
  \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and satisfying assignments $\boldsymbol{x}$ and $\boldsymbol{y}$ on input and output layers respectively\\
  \textbf{Output: } marked neurons or real counter example. 
  \begin{algorithmic}[1]
    \State \textbf{return} $\boldsymbol{x}$ if $N(\boldsymbol{x}) \models Q$. 
    \State Let us say $\boldsymbol{val_{i}}$ is the value vector at layer $l_i$, when $\boldsymbol{x}$ executed on network $N$. 
    \For{$i=1$ to $k$} \Comment{inputLayer excluded}
      \If{$l_i$ is affine layer}
        \State $\boldsymbol{val_i} = W_i * \boldsymbol{val_{i-1}} + B_i$ \Comment{simple matrix muliplication}
      \Else \Comment{Relu layer}
        \State constraints = \{\}
        \For{$t=i$ to $k$}\Comment{Each layer after $l_i$}
          \For{$j=0$ to $|l_t|$}
            \State constrains.add($A(n_{tj}).lexpr \leq x_{tj}\leq A(n_{tj}).uexpr$)
          \EndFor
        \EndFor
        \For{$j=0$ to $|l_{i-1}|$}\Comment{Previous layer}
          \State constrains.add($x_{i-1,j} == val_{i-1,j}$)
        \EndFor
        \For{$j=0$ to $|l_k|$}\Comment{Output layer}
          \State constrains.add($x_{kj} == y_{j}$)
        \EndFor
        softConstraints = \{\}
        \For{$j=0$ to $|l_i|$}\Comment{Current layer}
          \State softConstraints.add($x_{ij} == val_{ij}$)
        \EndFor
        \State maximize the softConstraints with satisfying the constrains. 
        \If{all the softConstraints satisfied}
          \State \textbf{continue}
        \Else
          \State markedNeurons = all neurons of layer $l_i$ which does not satisfy the softConstraints. 
          \State \textbf{return} markedNeurons
        \EndIf 
      \EndIf
    \EndFor
  \end{algorithmic}
  \caption{An optimization based approach to get mark neurons or counter example}
  \label{algo:refine2}
\end{algorithm}






%--------------------- DO NOT ERASE BELOW THIS LINE --------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
