We define the notions and definitions in this section. 
These notions and definitions are used in the later part of the paper. 
The network is defined in the definition \ref*{def:net}. 

% l_{in}, l_{out}
\begin{df}
    \label{def:net}
    A neural network $N = (Neurons, Layers, Edges, W, B, LayerType)$ is a tuple, where
    \begin{itemize}
        \item $Neurons$ is the set of neurons in $N$,
        \item $Layers = \{l_0,...,l_k\}$ is an indexed partition of $Neurons$,
        % \item $l_i \in Layers$ represents the $i^{th}$ layer in network $N$. 
        % \item $|l_i|$ represents the number of neurons in layer $l_i$. 
        % \item 
        \item $ Edges \subseteq \Union_{i=1}^{k} l_{i-1} \times l_{i}$
        \item $W : Edges \mapsto \reals$
        \item $B : Neurons \mapsto \reals$
        \item $LayerType : Layers \mapsto \{\mathtt{Affine}, \mathtt{Relu}\}$
    \end{itemize}
\end{df}

A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents the number of layers.
We call $l_0$ and $l_k$ the {\em input} and {\em output layers} respectively, and all the other layers 
as {\em hidden layers}. We assume a separate layer for activation function. Though there are different kinds of
activation, but we focus only on \relu{}, hence each layer can either be an affine layer or an \relu{} layer.
There is a one to one mapping between an affine layer $l_{i-1}$ and a \relu{} layer $l_i$. 
% If a layer $l_i$ is \relu{} layer, then the input neuron of $n_{ij}$ will only be the neuron $n_{(i-1)j}$. 
Let us say a vector $\boldsymbol{val_i} = [val_{i0}, val_{i1}, ... val_{im}]$ represents the values of each neuron 
in the layer $l_i$, where $m$ is the number of nodes in the same layer.
The value of $\boldsymbol{val_i}$ computed by the weighted sum of the previous layer's values($W_i * V_{i-1} + B_i$)
if $l_i$ is affine layer, otherwise $\boldsymbol{val_i}$ computed by the \relu{} function. 
A function $y = max(0,x)$ is a \relu{} function which takes an arguments $x$ as input and return the
same value $x$ as output if $x$ is non-negative otherwise return the value 0. 

A neural network can be visualize as a function $N$ which takes an input of $m$ dimensions and gives an 
output of $n$ dimensions. $N$ can be represented as a composition of functions $f_k o f_{l-1} ... o f_1$,
where each function $f_i$ represents either the linear combinations of previous layer's
output or an activation function. 

% For any vector $\boldsymbol{v}$, $v_i$ represents it's $i^{th}$ value.  

% Let $\reals$ be the set of real numbers.
% Let $x_{\alpha}$ are unbounded set of real variables, where
% $\alpha$ is arbitrary index for variables.

\begin{df}
    \label{def:linexpr}
    $Linexpr = \{ w_0 + \sum_{i} w_i x_i | w_i \in \reals \land x_i \text{ is a real variable} \}$.
\end{df}
  
\begin{df}
    \label{def:linconstr}
    $Linconstr = \{expr \text{ op } 0 | expr \in LinExpr \land op \in \{\leq, = \}\}$
\end{df}






\begin{df}
  A matrix $W_i \in \reals^{|l_i|\times|l_{i-1}|}$ represents the weight matrix for layer $l_i$, where  
    $$
    W_i[t_1, t_2] = 
    \begin{cases}
      W(e) & e=(n_{(i-1)t_2}, n_{it_1}) \in Edges,\\
      0 & \text{otherwise.}\\
    \end{cases}
    $$
\end{df}

\begin{df}
    A matrix $B_i \in \reals^{|l_i|\times 1}$ represents the bias matrix for layer $l_i$. The entry $B_i[t,0] = B(n_{it})$, where $n_{it} \in Neurons$. 
\end{df}

\begin{df}
    For a neuron $x \in N.neurons$,
    an abstract constraint $A(x) = (lb,ub, lexpr, uexpr)$ is a tuple, where
    $lb \in \reals$ is lower bound on the value of $x$,
    $ub \in \reals$ is the upper bound on the value of  $x$,
    $lexpr \in LinExpr$ is the expression for the lower bound, and
    $uexpr \in LinExpr$ is the expression for the upper bound.
\end{df}

The abstract constraint $A$ is generated by the tool deeppoly~\cite{} as explained in subsection \ref*{sec:deeppoly}. 


\subsection*{DeepPoly}
\label{sec:deeppoly}
Our abstract refinement approaches rely on the method deeppoly,
which is state-of-the-art in abstraction-based methods. Deeppoly uses the combination of
polyhedra~\cite{} and box~\cite{} domain. The core idea of it is to maintain an upper and lower linear
expression as well as the upper and lower bounds for each neurons. The upper and lower
expression uses the variables from the predecessor layer only. Globally deeppoly forms a
polyhedron. Experimentally, it has better precision in comparison to Box~\cite{} and Zonotope~\cite{}.
Deeppoly has the abstract transformer of various types of layers and activation functions. 
Currently, we are focusing on the affine layer and relu activation function only. 

The abstract constraint $A$ generates as follow: 
\begin{itemize}
    \item For an affine neuron $x_{ij}$, we set 
            $A(x_{ij}).lexpr = A(x_{ij}).uexpr = \sum_{t=1}^{|l_{i-1}|} W[j,t]*x_{(i-1)t} + B_i[j,0]$. Which is just an affine expression.
            They compute the value of $A(x_{ij}).lb$ and $A(x_{ij}).ub$ by back substituting the variables 
            in $A(x_{ij}).lexpr$ and $A(x_{ij}).uexpr$ respectively up to input layer.  
    \item For a relu neuron $x_{ij} = max(0,x_{(i-1)j})$, we consider three cases:
            \begin{enumerate}
                \item If $A(x_{(i-1)j}).lb \geq 0$ then relu is in active phase and $A(x_{ij}).lexpr = A(x_{ij}).uexpr = x_{(i-1)j}$,
                        and $A(x_{ij}).lb = A(x_{(i-1)j}).lb$ and $A(x_{ij}).ub = A(x_{(i-1)j}).ub$
                \item If $A(x_{(i-1)j}).ub \leq 0$ then relu is in passive phase and $A(x_{ij}).lexpr = A(x_{ij}).uexpr = 0$, 
                        and $A(x_{ij}).lb = A(x_{ij}).ub = 0$
                \item  If $A(x_{(i-1)j}).lb < 0$ and $A(x_{(i-1)j}).ub > 0$ then the behavior of relu is uncertain, and authors
                        do over-approximation. $A(x_{ij}).uexpr = u(x_{(i-1)j} - l) / (u - l)$, 
                        where $u = A(x_{(i-1)j}).ub \text{ and } l = A(x_{(i-1)j}).lb$.
                        And $A(x_{ij}).lexpr = \lambda . x_{(i-1)j}$, where $\lambda \in \{0,1\}$. 
                        They are choosing the value of $\lambda$ dynamically. They compute the value of $A(x_{ij}).lb$ and $A(x_{ij}).ub$ 
                        by back substituting the variables in $A(x_{ij}).lexpr$ and $A(x_{ij}).uexpr$ respectively up to input layer. 
            \end{enumerate} 
\end{itemize}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
