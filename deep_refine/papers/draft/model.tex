For any vector $\boldsymbol{v}$, $v_i$ represents it's $i^{th}$ value.  

Let $\reals$ be the set of real numbers.
Let $x_{\alpha}$ are unbounded set of real variables, where
$\alpha$ is arbitrary index for variables.

\begin{df}
    $Linexpr = \{ w_0 + \sum_{i} w_i x_i | w_i \in \reals \land x_i \text{ is a real variable} \}$.
\end{df}
  
\begin{df}
    $Linconstr = \{expr \text{ op } 0 | expr \in LinExpr \land op \in \{\leq, = \}\}$
\end{df}

A {\em predicate} is a Boolean combination of $Linconstr$.

% l_{in}, l_{out}
\begin{df}
    A neural network $N = (Neurons, Layers, Edges, W, B, LayerType)$ is a tuple, where
    \begin{itemize}
        \item $Neurons$ is the set of neurons in $N$,
        \item $Layers = \{l_0,...,l_k\}$ is an indexed partition of $Neurons$,
        % \item $l_i \in Layers$ represents the $i^{th}$ layer in network $N$. 
        % \item $|l_i|$ represents the number of neurons in layer $l_i$. 
        % \item 
        \item $ Edges \subseteq \Union_{i=1}^{k} l_{i-1} \times l_{i}$
        \item $W : Edges \mapsto \reals$
        \item $B : Neurons \mapsto \reals$
        \item $LayerType : Layers \mapsto \{\mathtt{Affine}, \mathtt{Relu}\}$
    \end{itemize}
\end{df}

A neural network is a collection of layers $l_0, l_1, l_2, ... l_k$, where $k$ represents the number of layers.
We call $l_0$ and $l_k$ the {\em input} and {\em output layers} respectively, and all the other layers 
as {\em hidden layers}. We assume a separate layer for activation function. Though there are different kinds of
activation, but we focus only on \relu{}, hence each layer can either be an affine layer or an \relu{} layer.
There is a one to one mapping between an affine layer $l_{i-1}$ and a \relu{} layer $l_i$. 
% If a layer $l_i$ is \relu{} layer, then the input neuron of $n_{ij}$ will only be the neuron $n_{(i-1)j}$. 
Let us say a vector $\boldsymbol{val_i} = [val_{i0}, val_{i1}, ... val_{im}]$ represents the values of each neuron 
in the layer $l_i$, where $m$ is the number of nodes in the same layer.\todo{Clearly Define transformations for relu layer and affine layers. Include assumptions on the shape of the neural network.}
The value of $\boldsymbol{val_i}$ computed by the weighted sum of the previous layer's values($W_i * V_{i-1} + B_i$)
if $l_i$ is affine layer, otherwise $\boldsymbol{val_i}$ computed by the \relu{} function. 
A function $y = max(0,x)$ is a \relu{} function which takes an arguments $x$ as input and return the
same value $x$ as output if $x$ is non-negative otherwise return the value 0. 

A neural network can be visualize as a function $N$ which takes an input of $m$ dimensions and gives an 
output of $n$ dimensions. $N$ can be represented as a composition of functions $f_k o f_{l-1} ... o f_1$,
where each function $f_i$ represents either the linear combinations of previous layer's
output or an activation function. 



\begin{df}
  A matrix $W_i \in \reals^{|l_i|\times|l_{i-1}|}$ represents the weight matrix for layer $l_i$, where
  % the dimension of $W_i$ is $|l_i|$ and $|l_{i-1}|$. 
  % Let $W_i[t_1, t_2] = W(e)$ if $e = (n_{(i-1)t_2}, n_{it_1}) \in Edges$. Otherwise, $W_i[t_1, t_2] = 0$.
    % \text{Let }
  
    $$
    W_i[t_1, t_2] = 
    \begin{cases}
      W(e) & e=(n_{(i-1)t_2}, n_{it_1}) \in Edges,\\
      0 & \text{otherwise.}\\
    \end{cases}
    $$
\end{df}

\begin{df}
    A matrix $B_i \in \reals^{|l_i|\times 1}$ represents the bias matrix for layer $l_i$. The entry $B_i[t,0] = B(n_{it})$, where $n_{it} \in Neurons$. 
\end{df}

% We write $v_{ij} \in l_i$ to denote the $j^{th}$ neuron of layer $l_i$.
% We call $l_0$ as input layer and $l_k$ as the output layer.
% Each layer $l_i$ is a collection of nodes.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
