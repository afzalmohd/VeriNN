\subsection{Proofs of progress and termination}
Our refinement strategy ensures progress, i.e., the spurious counterexample does not repeat in the future iterations of Algorithm~\ref{algo:main}. Let us suppose the algorithm
$\textsc{getMarkedNeurons}$ gets the abstract spurious counterexample
${v_0}, {v_1}, ... {v_k}$ and
returns marked neurons in some iteration of the while loop, say $i^{th}$-iteration.
The call to $\maxsat{}$ at line 10
declares that $constr \land softsatSet$ is satisfiable.
We can extract an abstract spurious counterexample from a model
of $constr \land softsatSet$.
Let $m$ be the model.
Let the abstract spurious counterexample be $cex = val_0^{v_0}, ...., val^{v_0}_{i-1},v'_{i},...,v'_{k-1},v_k$.
Before the iteration $i$, $cex$ follows the execution of $N$ on input $v_0$.
After the iteration $i$, we use the model to construct $cex$, i.e., $m(x_i) = v'_i$.

%\begin{theorem}
  \begin{lemma}
  In the rest of run of Algorithm~\ref{algo:main}, i.e., future iterations of the while loop, $\textsc{IsVerified}$ will not return the abstract spurious counterexample  $cex$ again.
  %\end{theorem}
  \end{lemma}
\begin{proof}
  For $n_{ij} \in newMarked$, the \maxsat{} query ensures that
  $val^{v_0}_{ij} \neq v'_{ij}$.
  If we have the same counterexample again in the future then
  input of $n_{ij}$ will be $val^{v_0}_{(i-1)j}$.
  Since we will have exact encoding for $n_{ij}$, the output
  will be $val^{v_0}_{ij}$, which contradicts the earlier inequality.  
\end{proof}



% Suppose the algorithm {\em getMarkedNeurons} get abstractCEX 
% ${v_0}, {v_1}, ... {v_k}$ as input, 
% and ${val_i^{{v_0}}}$ represents the value vector on layer $l_i$, 
% when we execute the neural network on input ${v_0}$.


Next, we turn to termination of the algorithm. We have two lemmmas.

\begin{lemma}
  \label{th:progress1}
  In every refinement iteration $\textsc{getMarkedNeurons}$
  returns a non-empty set of marked neurons. 
\end{lemma}

\begin{proof}
By the definition of abstract spurious counterexample, ${v_k} \models \lnot Q$. 
By the check at line 6 of Algorithm~\ref{algo:main} ${val_k^{{v_0}}} \models Q$. 
If the set of returned new marked neurons is empty, $newMarked = \emptyset$ for each layer. 
Therefore, all the neurons in any layer $l_i$ become equal to ${val_i^{{v_0}}}$,  
which implies ${v_k}$ equals to ${val_k^{{v_0}}}$, but ${v_k} \models \lnot Q$ and 
${val_k^{{v_0}}} \models Q$, which is a contradiction.   
\end{proof}
% {\em Proof by contradiction:} 


\begin{lemma}
  \label{th:progress2}
  In every refinement iteration $\textsc{getMarkedNeurons}$
  returns marked neurons, which were not marked in previous iterations. 
\end{lemma}

\begin{proof}
We will show that if a neuron $n_{ij}$ got marked in $t^{th}$ iteration then $n_{ij}$ will not be marked again in any iteration greater than $t$.
Consider an iteration $t' > t$, if we get the marked neurons from layer other
than $l_i$ then $n_{ij}$ can not be part of it because $n_{ij}$ is in layer $l_i$. 
Consider the case where marked neurons are from the layer $l_i$ in $t'^{th}$ iteration. 
Since we have made $n_{ij}$ exact in line 6 of Algorithm~\ref{algo:refine2},
its behavior while optimizing constraints will be same as the exact \relu{}. 
Moreover, $v'_{ij} = val_{ij}^{v_0}$, 
which implies the soft constraint for neuron $n_{ij}$ will always be satisfied. Hence it will not occur as a marked neuron as per the criteria of new marked neurons in line 10 of Algorithm~\ref{algo:refine2}.   %\todo{check last line.}
\end{proof}


Lemmas \ref{th:progress1} and \ref{th:progress2} imply that in every iteration $\textsc{getMarkedNeurons}$ returns a nonempty set of unmarked neurons, which will now be marked. In worst case, the algorithm will mark all the neurons of the network, and encode them in the exact behavior. Thus, we conclude,

\begin{theorem}
  Algorithm~\ref{algo:main} always terminates.
\end{theorem}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
