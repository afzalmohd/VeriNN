% \onecolumn

\begin{center}
    \Large{\bf{Appendix}}    
\end{center}

\section{Experiments}
\subsection{Comparison with PGD's adversarially trained network}
\label{ap:exp:1}

The networks considered for evaluation in this study are the ones corresponding to the 9th, and 10th 
rows of Figure~\ref{tb:nndetail}. These networks have been trained using adversarial techniques, 
where adversarial examples were generated using PGD attack, and the network was subsequently 
trained on these adversarial examples to enhance its robustness. Figure~\ref{tb:matrix2} presents a 
pairwise comparison of verifiers on these adversarially trained networks, 
encompassing a total of 1456 benchmark instances. 
Our approach demonstrates significant superiority over \deeppoly{}, \kpoly{}, and \deepsrgr{} in terms of 
performance on these benchmarks. While \alphabeta{} and \ovaltool{} outperform our approach by approximately 
90 benchmarks, we are still able to solve 75 unique benchmarks that remain unsolved by both of these tools. 
Considering the total number of benchmarks is 1456, this indicates a notable number of benchmarks that our 
approach successfully addresses.

\begin{figure}[t]
  \scriptsize
    \centering
    \begin{tabular}{|c|c|c|c|c|c|g||c|}
        \hline
        \diagbox{\tiny Verified}{\tiny \mbox{}\hspace{-2mm}Unverified} & \tiny \textbf{\deeppoly} & \tiny \textbf{\kpoly} & \tiny \textbf{\deepsrgr} & \tiny \textbf{\alphabeta} & \tiny \textbf{\ovaltool} & \tiny \textbf{\drefine} & \tiny \textbf{\textsc{total}} \\
        \hline
        \tiny \textbf{\deeppoly} & \textbf{0} & 0 & 0 & 3 & 3 & 0 & 913 \\
        \hline
        \tiny \textbf{\kpoly} & 9 & \textbf{0} & 9 & 5 & 4 & 2 &  922 \\ 
        \hline
        \tiny \textbf{\deepsrgr} & 2 & 2 & \textbf{0} & 3 & 3 & 0 & 915 \\ 
        \hline
        \tiny \textbf{\alphabeta} & 250 & 243 & 250 & \textbf{0} & 1 & 163 & 1160 \\ 
        \hline
        \tiny \textbf{\ovaltool} & 258 & 259 & 258 & 9 & \textbf{0} & 170 & 1168 \\
        \hline
        \rowcolor{green!20}
        \tiny \textbf{\drefine} & 170 & 163 & 168 & 76 & 75 & \textbf{0} & 1073 \\
        \hline
    \end{tabular}
    % \caption{Pairwise comparison of tools on adversarially trained networks, e.g. entry on row \kpoly{} and column \deeppoly{} represents 9 benchmark instances on which \kpoly{} verified but \deeppoly{} fails. The green row highlights the number of solved benchmark instances by \drefine{} and not others while the red column is the opposite.}
    \caption{Pairwise comparison of tools on adversarially trained networks}
    \label{tb:matrix2}
\end{figure}





% \subsubsection{Pullback approach: }

% Suppose the abstractCEX is $\boldsymbol{v_0}, \boldsymbol{v_1}, ... , \boldsymbol{v_k}$. 
% The core idea of this approach is to find a point $\boldsymbol{p_{k-1}}$ in the layer $l_{k-1}$. 
% The point $\boldsymbol{v_k}$ is guaranteed to be reachable from point $\boldsymbol{p_{k-1}}$ in the concrete domain.
% Similarly, we find points $\boldsymbol{p_{k-3}}, \boldsymbol{p_{k-5}}, ... \boldsymbol{p_0}$, 
% such that $p_i$ is always reachable from point $p_{i-2}$. 
% We find these points on each $\relu${} layer and input layer only. 
% If we find the point $\boldsymbol{p_0}$ in the input layer, it means $\boldsymbol{p_0}$ is a counter-example, 
% because we can reach from $\boldsymbol{p_0}$ to $\boldsymbol{p_2}$, $\boldsymbol{p_2}$ to $\boldsymbol{p_4}$ 
% , and so on up to $\boldsymbol{v_k}$. 
% If we get stuck in some layer $l_i$ i.e. fails to find point 
% $\boldsymbol{p_{i-2}}$. It means point $\boldsymbol{p_i}$ does not have its corresponding point $\boldsymbol{p_{i-2}}$, 
% which implies that point $\boldsymbol{p_i}$ is a spurious point generated by $\relu${} layer $l_i$. 
% The algorithm \ref{algo:refine1} compute such points. In line number $2$, we equate the value of 
% each element of $\boldsymbol{v_k}$ to the corresponding neuron's affine expression($lexpr$ or $uexpr$), 
% and take the conjunction, and check satisfiability. Since the affine expression of each neuron in $l_k$ contains the 
% variable of layer $l_{i-1}$, so, the satisfying assignment is the point $p_{k-1}$. Similarly, we build the constraints
% for each hidden affine layer's neurons. For a neuron $n_{ij}$ of affine layer $l_i$, 
% if the corresponding point's value $p_{i+1}(j)$ is greater than $0$ then we equate the affine expression of $x_{ij}$ to $p_{i+1}(j)$,
% otherwise, we set the lower and upper bound of the affine expression of $x_{ij}$ as $A(x_{ij}).lb$ and $0$ respectively. 
% Which is the replication of $\relu${} function $x_{(i+1)j} = max(0, x_{ij})$. We construct such constraint 
% for each neuron of $l_i$, and build a formula by taking the conjunction
% of each neuronâ€™s constraint and checking the satisfiability of this formula.
% If it is satisfiable then the point $\boldsymbol{p_{i-1}}$ is found, 
% otherwise, we get the $\mathtt{unsat}$core. We collect all the neurons of $l_i$ whose constraints are 
% in $\mathtt{unsat}$core, and return them as the markedNeurons.   



% \begin{algorithm}[t]
%     \textbf{Name: } pullback \\
%     \textbf{Input: } $\langle N,P,Q \rangle$, abstract constraints $A$ and abstractCEX $=\boldsymbol{v_0}, \boldsymbol{v_1}, .., \boldsymbol{v_k}$ \\
%     \textbf{Output: } markedNeurons or cex. 
%     \begin{algorithmic}[1]
%      \State \textbf{return} $\boldsymbol{v_0}$ if $N(\boldsymbol{v_0}) \models \neg Q$. 
%      \State $constr := \Land_{j=1}^{|l_k|} (A(x_{kj}).lexpr = v_k(j))$
%      \State isSat = checkSat(constr)
%      \If{isSat} \Comment{must be true, last affine layer dont add spurious information}
%         \State $\boldsymbol{p_{k-1}} = \boldsymbol{satval_{k-1}}$ 
%      \EndIf
%      \For{$i=k-1$ to $1$}
%         \If{$l_i$ is affine layer}
%           \State $layerConstraints := true$
%           \For{$j=1$ to $|l_i|$}
%             \If{$p_{i+1}(j) > 0$}
%               \State $constr_{ij}$ := $(A(x_{ij}).lexpr = p_{i+1}(j)$) \Comment{lexpr=uexpr for affine}
%             \Else
%               \State $constr_{ij}$ := $(A(x_{ij}).lb \leq A(n_{ij}).lexpr \leq 0$)
%             \EndIf
%             \State $layerConstrains := layerConstrains \land constr_{ij}$
%           \EndFor
%           \State isSat = checkSat(layerConstrains)
%           \If{not isSat}
%             \State markedNeurons = \{$n_{ij}$ | $1 \leq j\leq |l_i| \land constr_{ij}$ $\in$ unsatCore\}
%             \State \textbf{return } markedNeurons
%           \Else
%             \State $\boldsymbol{p_{i-1}} = \boldsymbol{satval_{i-1}}$
%           \EndIf
%         \EndIf
%      \EndFor
%       \State \textbf{return} $\boldsymbol{p_0}$ \Comment{cex if pullbacked till input layer}
%     \end{algorithmic}
%     \caption{A pullback approach to get mark neurons or counter example}
%     \label{algo:refine1}
%   \end{algorithm}



% \subsection{Utilizing of spurious information}
% The \emph{isVerified} function in algorithm~\ref{algo:main} calls either algorithm~\ref{algo:verif1} or algorithm~\ref{algo:verif2}. 
% Both algorithms~\ref{algo:verif1} and \ref{algo:verif2} take \markednewrons{} as input. 
% The \markednewrons{} represent the set of the culprit neurons. 
% Algorithm~\ref{algo:verif2} splits each neuron of \markednewrons{} into two sub-cases. Suppose the 
% neuron $x\in [lb,ub]$ belongs to \markednewrons{}, then the first case is when $x \in [lb,0]$, and the second case is when $x \in [0,ub]$.   
% After splitting neurons into two cases \deeppoly{} runs for both cases separately. In algorithm \ref{algo:verif2}, \deeppoly{}
% runs an exponential number of times in the size of the \markednewrons{}. We return verified in algorithm~\ref{algo:verif2} if 
% the verification query is verified in all the \deeppoly{} runs. If \deeppoly{} fails to verify in any case then we return the abstractCEX. 

% \begin{algorithm}[t]
%   \textbf{Name: } isVerified2 \\
%   \textbf{Input: } $\langle N,P,Q \rangle$ with abstract constraints $A$ and markedNeurons \\
%   \textbf{Output: } verified or  abstractCEX. 
%   \begin{algorithmic}[1]
%     \For{all combination in $2^{markedNeurons}$}
%       \State $A'$ = run deeppoly
%       \State $constr := P \land (\Land_{i=1}^k lc'_i) \land \neg Q$ \Comment{$lc'$ is with respect to $A'$}
%       \State isSat = checkSat(constr)
%       \If{isSat}
%         \State \textbf{return} abstractCEX
%       \EndIf
%     \EndFor
%     \State \textbf{return} verified
%   \end{algorithmic}
%   \caption{An approach to verify $\langle N,P,Q \rangle$ with abstraction A}
%   \label{algo:verif2}
% \end{algorithm}




% \textbf{Pullback approach}\\
% % The core idea of this approach is to find a point in the predecessor layer, such that the current point 
% % can be reached from the predecessor layers point. 
% This approach finds a point of $x_7=2,x_8=1$ in the predecessor layer means the value of $x_5$ and $x_6$. 
% The point $x_7=2$ and $x_8=1$ must reach from the values of $x_5$ and $x_6$. 
% We compute the value by using the \sat{} query on the affine layer. The \sat{} query shown 
% in equation~\ref{eq:back1}, 
% build by using the values and affine expression of $x_7$, $x_8$, and by using the bounds of $x_5$ and $x_6$.
% The satisfying value of equation~\ref{eq:back1} is $x_5=2$ and $x_6=0$, which is the predecessor point of $x_7=2$
% and $x_8=1$.  
% \begin{equation}
%     \begin{aligned}
%         x_7 = 2 & \land x_8 = 1 \\
%         x_5 = 2 & \land x_6 + 1 = 1 \\ 
%         x_5=2\land x_6+1 = 1 & \land 0\leq x_5 \leq 2 \land 0\leq x_6 \leq 2 \\
%     \end{aligned}
% \label{eq:back1}
% \end{equation}

% In this approach, we compute the predecessor point by considering both the \texttt{affine} and \texttt{relu} layers together. 
% We compute the predecessor point of $x_5=2, x_6=0$ in terms of $x_1$ and $x_2$.
% Since $x_6=0$ and $x_6=max(0,x_4)$ then $x_4$ can be anything from its lower bound to $0$ i.e. $-2 \leq x_4 \leq 0$.
% Similarly the value of $x_5=2$ and $x_5=max(0,x_3)$ then the value of $x_3$ will be $2$. 
% The \sat{} query build as shown in equation~\ref{eq:back2}. 

% \begin{equation}
%     \begin{aligned}
%         x_5 = 2 & \land x_6 = 0 \\
%         x_3 = 2 & \land -2\leq x_4 \leq 0 \\ 
%         x_1+x_2=2\land -2\leq  x_1+x_2 \leq 0 & \land -1\leq x_1 \leq 1 \land -1\leq x_2 \leq 1 \\
%     \end{aligned}
% \label{eq:back2}
% \end{equation}

% When we check the satisfiability of equation~\ref{eq:back2}, it returns \unsat{}. It means the predecessor point of 
% $x_5=2,x_6=0$ does not exist, which implies that point $x_5=2, x_6=0$ is introduced by the triangle approximation.
% Whenever the solver returns \unsat{}, we also get the \unsatcore{}~\ref{sec:solver}. In this example we get the 
% \unsatcore{}=$\{x_1+x_2=2, -2\leq  x_1+x_2 \leq 0\}$, so, we mark the corresponding neurons $x_3$ and $x_4$ as a marked neurons. 
% And refine the marked neurons in the refinement procedure. 


% \textbf{Refinement} \\
% We have two approaches for the refinement {\em Splitting based approach} and {\em MILP-based approach}.
% We elaborate here the {\em MILP-based approach} only. The other approach can be seen in algorithm~\ref{algo:verif2}. 
% These two approaches refine the marked neurons. We got two different sets of marked neurons in the above analysis. 
% The {\em pullback} approach returns $\{x_5, x_6\}$ as the marked neurons, while {\em optimization-based approach}
% return $\{x_6\}$ as the marked neurons. In the following analysis, we are taking the marked neurons set as $\{x_5, x_6\}$.
% Although the analysis will also work if we take the other set of marked neurons.

% In {\em MILP based approach}, we add the exact encoding of the marked neuron ($x_5, x_6$) in addition to the constraints
% in equation~\ref{eq:conjunction} and check the satisfiability, now it becomes \unsat{}, hence, the property verified. 
% The exact constraint of a $\relu${} neuron is explained in equation~\ref{eq:reluexact}. 

% The {\em Splitting based approach} splits the bounds at $0$ of the incoming neurons of the marked neurons. 
% The incoming neurons of $x_5$ and $x_6$ are $x_3$ and $x_4$ respectively. 
% The affine expression of both the incoming neurons is $x_1+x_2$. 
% When we split both $x_3$ and $x_4$ then four cases get generated. The property should get verified in all four cases. 
% All the cases are explained as follows: 
% % Suppose I split $x_3$ then two cases generated
% % $x_3 > 0$ and $x_3 \leq 0$, which implies $x_1+x_2 > 0$ and $x_1+x_2 \leq 0$ respectively. 
% % When we split both $x_3$ and $x_4$ then four cases get generated. Out of four cases $x_3 > 0 \land x_4 \leq 0$ and 
% % $x_3 \leq 0 \land x_4 > 0$ are infeasible cases, because the corresponding affine expression 
% % $x_1+x_2 > 0 \land x_1 + x_2 \leq 0$ is infeasible. We analyze the remaining tow cases as follow: 
% \begin{itemize}
%     \item $x_3 > 0$ and $x_4 > 0$, since neurons are in active phase, so, upper and lower expressions for $x_5$ 
%     becomes $x_3$, similarly the upper and lower expression for $x_6$ becomes $x_4$. 
%     The expression for $x_7$ and $x_8$ remains the same. The upper bound of $x_7 - x_8$ computed in equation~\ref{eq:split1} is $-1$. 
%     The property got verified.   
%     \item $x_3 \leq 0$ and $x_4 \leq 0$, since both neurons are in passive phase, so, the upper and lower expressions 
%         for both the neurons become $0$. The upper and lower expressions of the other neurons remain same. 
%         The upper bound of $x_7 - x_8$ is computed $-1$ in equation~\ref{eq:split2}. The property got verified here too. 
%     \item $x_3 \leq 0$ and $x_4 > 0$, is infeasible case because $x_1+x_2 \leq 0 \land x_1 + x_2 > 0$ is infeasible. 
%     \item $x_3 > 0$ and $x_4 \leq 0$, is infeasible case because $x_1+x_2 > 0 \land x_1 + x_2 \leq 0$ is infeasible. 
% \end{itemize}

% \begin{equation}
%     \begin{aligned}
%         x_7 - x_8 \leq & x_5 - x_6 - 1 \\
%        \leq & x_3 - x_4 -1 \\
%        \leq & x_1 + x_2 -(x_1+x_2) -1 \\
%        \leq & -1 
%     \end{aligned}
%     \label{eq:split1}
% \end{equation}

% \begin{equation}
%     \begin{aligned}
%       x_7 - x_8 \leq & x_5 - x_6 - 1 \\
%         \leq & 0 - 0 - 1 \\
%         \leq & -1
%     \end{aligned}
%     \label{eq:split2}
% \end{equation}


