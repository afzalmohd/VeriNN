\begin{figure}[t]
	\centering
	\scalebox{0.8}{\input{fig/nn1.tex}}
	\caption{Hypothetical example of neural network}
	\label{fig:motivating}
\end{figure}
% Consider a neural network in Figure~\ref{fig:motivating},  which has one input, one hidden, and one output layer. The hidden layer is separated into two layers 
% \affine{} and \relu{}, so a total of four layers is shown in Figure~\ref{fig:motivating}. 
Consider the neural network depicted in Figure~\ref{fig:motivating}, which comprises one input layer, one hidden layer, 
and one output layer. The hidden layer is divided into two sub-layers: \affine{} and \relu{}, 
resulting in a total of four layers shown in Figure~\ref{fig:motivating}.
Every layer contains two neurons. The neuron $x_8$ has a bias of $1$, and all the other neurons have a bias of $0$. 
Our goal is to verify for all input $x_1,x_2 \in [-1,1]$ the outputs satisfy $x_7 \leq x_8$. 
Our approach extends \deeppoly{}~\cite{singh2019abstract}.
\deeppoly{} maintains one upper and
one lower constraint and an upper and lower bound for each neuron.
For a neuron of the affine layer, the upper and lower constraint is 
the same, which is the weighted sum of the input neurons i.e. $x_3$'s upper and lower constraint is $x_1+x_2$.
For an activation neuron, the upper and lower expression is computed using triangle approximation~\cite{singh2019abstract}, 
which is briefly explained in Section~\ref{sec:deeppoly}. To verify the property $x_7 \leq x_8$, \deeppoly{} creates a 
new expression $x_9 = x_7 - x_8$ and computes the upper bound of $x_9$. The upper bound of $x_9$ should not be greater
than $0$. \deeppoly{} computes the upper bound of $x_9$ by back substituting the expression of $x_7$ and $x_8$ 
from the previous layer.
They continue back substituting until only input layer variables are left.
The process of back substitution is shown in Equation~\ref{eq:deeppoly}.
After back substitution, the upper bound of $x_9$ is
computed as $1$, which is greater than $0$, 
hence, the \deeppoly{} fails to verify the property.

% \hspace*{-1cm}
% \fbox{
% \noindent\begin{minipage}{.23\linewidth}
% \begin{equation*}
%     \begin{aligned}
%         x_9 \leq  &  x_7 - x_8 &\\
%         x_9 \leq  & x_5 - x_6 - 1 \\
%         x_9 \leq  & 0.5x_3 + 1 - 1 &\\
%         x_9 \leq  & 0.5(x_1+x_2) \\
%         x_9 \leq  & 1\\
%         (1)
%    \end{aligned}
% %\label{eq:deeppoly}
% \end{equation*}
% \end{minipage}}\quad
% \fbox{
% \begin{minipage}{.68\linewidth}
% \begin{equation*}
%     \begin{aligned}
%       -1 \leq & x_1 \leq 1 \hspace*{-1cm}&  \hspace*{-2cm}
%       -1 \leq & x_2 \leq 1 \\
%          x_1 + x_2 \leq & x_3 \leq x_1 + x_2 \hspace*{-1cm}& \hspace*{-2cm}
%          x_1 + x_2 \leq & x_4 \leq x_1 + x_2 \\
%          0 \leq & x_5 \leq 0.5x_3+1 \hspace*{-1cm}&  \hspace*{-2cm}
%          0 \leq & x_6 \leq 0.5x_4 + 1 \\
%          x_5 \leq & x_7 \leq x_5 \hspace*{-1cm}&\hspace*{-2cm}
%          x_6+1 \leq & x_8 \leq x_6+1 \\
%          x_7 & > x_8 \text{ (negation of property)}&&\\
%         & \hspace*{2cm}(2) &&
%     \end{aligned}
% \label{eq:conjunction}
% \end{equation*}

% \end{minipage}
% }  

\begin{equation}
    \begin{aligned}
        x_9 \leq  &  x_7 - x_8 &\\
        x_9 \leq  & x_5 - x_6 - 1 \\
        x_9 \leq  & 0.5x_3 + 1 - 1 &\\
        x_9 \leq  & 0.5(x_1+x_2) \\
        x_9 \leq  & 1\\
   \end{aligned}
\label{eq:deeppoly}
\end{equation}

\begin{equation}
    \begin{aligned}
      -1 \leq & x_1 \leq 1 \hspace*{-1cm}&  \hspace*{-2cm}
      -1 \leq & x_2 \leq 1 \\
         x_1 + x_2 \leq & x_3 \leq x_1 + x_2 \hspace*{-1cm}& \hspace*{-2cm}
         x_1 + x_2 \leq & x_4 \leq x_1 + x_2 \\
         0 \leq & x_5 \leq 0.5x_3+1 \hspace*{-1cm}&  \hspace*{-2cm}
         0 \leq & x_6 \leq 0.5x_4 + 1 \\
         x_5 \leq & x_7 \leq x_5 \hspace*{-1cm}&\hspace*{-2cm}
         x_6+1 \leq & x_8 \leq x_6+1 \\
         x_7 & > x_8 \text{ (negation of property)}&&\\
    \end{aligned}
\label{eq:conjunction}
\end{equation}

There are two main reasons for the failure of \deeppoly{}. First, it cannot maintain the complete correlation 
between the neurons. In this example, neurons $x_3$ and $x_4$ have the same expression $x_1+x_2$, so they always
get the same value. However, in the \deeppoly{} analysis process, it may fail to get the same value. Second, it uses triangle
approximation on \relu{} neurons.
We take the conjunction of upper and lower expressions of each neuron with the negation of the property
as shown in Equation~\ref{eq:conjunction},
 and use the \milp{} solver to check satisfiability, thus addressing the first issue.  
The second issue can be resolved either by splitting the bound at zero of the 
affine node or by using the exact encoding (Equation~\ref{eq:reluexact}) 
instead of triangle approximation. 
But both solutions increase the problem size exponentially in terms of \relu{} neurons and this results in a huge 
blowup if we repair every neuron of the network. 

So, the main hurdle toward efficiency is to find the set of important neurons (we call these {\em marked neurons}), 
and only repair these.  For this, we crucially use the satisfying assignment obtained from the \milp{} solver.
%When \deeppoly{} fails to verify the network, we use an \milp{} solver to check the satisfiability of equation~\ref{eq:conjunction}. 
%If it returns \unsat{} it means the property is verified otherwise we get the satisfying assignment of each variable. 
For instance, a possible satisfying assignment of Equation~\ref{eq:conjunction}
is in Equation~\ref{eq:sat1}. We execute the neural network with the inputs $x_1=1,x_2=1$ and get the values 
on each neuron as shown in Equation~\ref{eq:sat2}. 
Then we observe that the output values $x'_7=2, x'_8=3$ satisfy the property, 
so, the input $x_1=1, x_2=1$ is a spurious counterexample. 
The question is to identify the neuron whose abstraction lead to this imprecision.
%\vspace{-20mm}
\setcounter{equation}{2}
\begin{align}
  x_1=1, x_2=1, x_3=2, x_4=2, x_5=2, x_6=0, x_7=2, x_8=1 \label{eq:sat1} \\
  x'_1=1, x'_2=1, x'_3=2, x'_4=2, x'_5=2, x'_6=2, x'_7=2, x'_8=3 \label{eq:sat2}
\end{align}

%We have to remove spurious counter example by doing the refinement analysis. 
%We are using one approach to find the marked neurons guided by the spurious counter example,  and one approach to refine (repair) the marked neurons.

% We are using two approaches to find the marked neurons guided by the spurious counter example, 
% and two approaches to refine (repair) the marked neurons.
% \begin{equation}
%     \begin{aligned}
%         x_1=1, x_2=1, x_3=2, x_4=2, x_5=2, x_6=0, x_7=2, x_8=1\\
%     \end{aligned}
%         \label{eq:sat1}
% \end{equation}

% \begin{equation}
%     \begin{aligned}
%         x'_1=1, x'_2=1, x'_3=2, x'_4=2, x'_5=2, x'_6=2, x'_7=2, x'_8=3
%     \end{aligned}
% \label{eq:sat2}
% \end{equation}


%Following is the approach to find the marked neurons.
%\\
\noindent\textbf{Maxsat based approach to identify marked neurons}\\
% To find the neurons whose abstraction leads to imprecision, let us see figure~\ref{fig:pictorial1}. 
% {\color{red} Here, $p_i$ represent the abstract constraint space in layer $l_i$ and } 
% black line represents the spurious counterexample denoted by equation~\ref{eq:sat1}. 
% The green line represents the exact execution of the input point of spurious counterexample 
% which is denoted by equation~\ref{eq:sat2}.  
% Our goal is to make the black line as close as possible to the green line from the first layer to the last layer, but the 
% first and last points should remain the same i.e., $x_1=1,x_2=1,$ and $x_7=2, x_8=1$, the closest line is the blue line.
% {\color{red} The point $v_i$ is vector of the values of neurons of layer $l_i$ of black line, similarly $v'_i$ and $v''_i$ represents 
% the vector of green and blue line.} 
% The blue line is also the abstract execution but it is closest to the exact execution of the spurious counter example. 
% {\color{red}\todo{As per comment 1.5.4 of reviewer 2} Here, the closeness measures are determined by comparing the equality of the corresponding neurons in abstract execution and exact execution.}
% Here the closeness measures in terms of the equality of the neurons. 
{\color{red}\todo{With respect to comment 6 of reviewer 1} To identify the neurons whose abstraction leads to imprecision, let us refer to Figure~\ref{fig:pictorial1}. 
In the figure, $p_i$ represents the abstract constraint space in layer $l_i$, while the black line denotes 
the spurious counterexample depicted in Equation~\ref{eq:sat1}. 
On the other hand, the green line represents the exact execution of the input point of the spurious counterexample, 
as denoted by Equation~\ref{eq:sat2}.

The objective is to make the black line as close as possible to the green line from the first layer to the last layer 
while keeping the first and last points the same, i.e., $x_1 = 1$, $x_2 = 1$, and $x_7 = 2$, $x_8 = 1$. 
The closest line to achieving this goal is represented by the blue line, 
which is also the abstract execution but exhibits the highest closeness to the exact execution of the spurious counterexample.
In this context, $v_i$ refers to the vector of values of neurons in layer $l_i$ of the black line, 
while $v'_i$ and $v''_i$ represent the vectors of the green and blue lines, respectively.
}

The green and black points are the same for the input layer, i.e., $[1,1]$. On the first affine layer, $l_1$ 
also, the black point $v_1$ is the same as the green point $v'_1$ since the affine layer does not introduce any spurious information. 
For $l_2$, we try to make $v''_2$ close to $v'_2$, such that $v''_2$ reaches to the $v_3$. We do that by encoding 
them as soft constraints (i.e.,  $\{x_5=2, x_6=2\}$) 
while maintaining that the rest of the hard constraints are satisfied (see Equation~\ref{eq:opt1})
e.g., input points $v_0=v''_0$ and output points $v_3=v''_3$ remain same. 
We mark the neurons of the layer where the blue line starts diverging from the green line, i.e., $l_2$. 
The divergence we find by the \maxsat{} query. If \maxsat{} returns all the soft constraints as satisfied, it means
the blue point becomes equal to the green point. If \maxsat{} returns partial soft constraints as satisfied, 
we mark the neurons whose soft constraints are not satisfied. In our example, \maxsat{} returns 
soft constraints $\{x_5=2\}$ as satisfied, it means soft constraint of $x_6$ could not satisfied, so, we mark $x_6$.
The blue and black lines are the same for our motivating example since it contains only one \relu{} layer. 
However, in general, it may or may not be the same. We optimize the blue line to be close to the green line while also resulting in as few marked neurons as  possible.




\noindent  
\fbox{
    \begin{minipage}{0.5\linewidth}
\begin{equation}
    \begin{aligned}
        x_1 = 1 & \land x_2 = 1 \\
        x_3 = x_1 + x_2 & \land x_4 = x_1 + x_2 \\
        0 \leq x_5 = 0.5x_3 + 1 & \land 0\leq x_6 \leq 0.5x_4 + 1 \\ 
        x_7 = x_5 & \land x_8 = x_6 + 1 \\
        x_7 = 2 & \land x_8 = 1
    \end{aligned}
    \label{eq:opt1}
\end{equation}
\end{minipage}
  }\;\;
  \begin{minipage}{0.42\linewidth}
%\textbf{Refinement} \\
%We have an approach for the refinement named as {\em MILP-based approach}.
Once we have  $x_6$ as the marked neuron, we use an {\em MILP based approach}, and add the exact encoding of the marked neuron ($x_6$) in addition to the constraints in Equation (2) %~\ref{eq:conjunction}
and check the satisfiability, now it becomes \unsat{}, hence, the property verified (see Equation~\ref{eq:reluexact} for more details).
%The exact constraint of a $\relu${} neuron is explained in 
\end{minipage}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.75]{fig/pictorial1.pdf}
    \caption{Pictorial representation of our approach on example in figure~\ref{fig:motivating}}
    \label{fig:pictorial1}
\end{figure}


% \begin{figure}[!ht]
% 	\centering
% 	\scalebox{0.55}{\input{fig/pictorial1.tex}}
% 	\caption{Pictorial representation of our approach}
% 	\label{fig:pictorial}
% \end{figure}





%% \begin{figure}
%%     \begin{minipage}{0.9\textwidth}
%%         \centering
%%         \includegraphics[scale=0.8]{fig/pictorial1.pdf}
        
%%         (a)
%%     \end{minipage}

%%     \begin{minipage}{0.9\textwidth}
%%         \centering
%%         \includegraphics[scale=0.8]{fig/pictorial2.pdf}

%%         (b)
%%     \end{minipage}
%%     \caption{Pictorial representation of our approach}
%%     \label{fig:pictorial}
%% \end{figure}


%% Consider the pictorial representation of our approach in figure~\ref{fig:pictorial}. This pictorial representation is not 
%% related to the motivating example of figure~\ref{fig:motivating}. The shapes $p_0, p_1, p_2$, and $p_4$ represent 
%% the abstract constraints on each layer. We took the rectangle shape for simplicity,  but it is not necessarily a rectangle shape. 
%% The red zone on the output layer shows the intersection of the abstract constraints and the negation of the property. If this 
%% intersection is empty, we can say that the property was verified. Otherwise, we will get a path 
%% $v_0\rightarrow v_1\rightarrow v_2\rightarrow v_3$ as shown in figure~\ref{fig:pictorial}(a), which may or may not be a counterexample. 
%% We execute $v_0$ on neural network
%% and get path $v_0\rightarrow v'_1\rightarrow v'_2\rightarrow v'_3$ as shown in figure~\ref{fig:pictorial}(a). 
%% If the point $v'_3$ reaches the red zone, 
%% we report $v_0$ as a counterexample. Otherwise, we refine the path $v_0\rightarrow v_1\rightarrow v_2\rightarrow v_3$. 
%% To refine, we make the black line close to the green line from the first layer to the last layer so that the endpoints($v_0, v_3$)
%% remain unchanged. Here the closeness means making the neuron's values equal. 
%% First, we make the points $v_1$ close to $v'_1$, and it become completely equal (all neuron's values of $v_1$ become equal to the 
%% corresponding neuron's values of $v'_1$), we move on the the layer $l_2$. 
%% On layer $l_2$,
%% we make the points $v_2$ close to $v'_2$, and some neurons of $v_2$ become equal to the corresponding neurons of $v'_2$, and some
%% neurons could not become equal, so we got a new point $v''_2$ close to $v'_2$.  
%% Finally we got the path $v_0\rightarrow v'_1\rightarrow v''_2\rightarrow v_3$ in figure~\ref{fig:pictorial}(b) 
%% close to the green line.
%% We pick the layer where the blue line diverges first from the green line. Furthermore, get the neurons whose values are different 
%% from the green point's neuron's values. In figure ~\ref{fig:pictorial}(b), the first layer where blue line diverges from the 
%% green line is $l_2$.
%% We find all the neurons of points $v''_2$ and $v'_2$ whose values differ and mark them.     


% \begin{equation}
%     \begin{align}
%         \text{softConstrs} = \{x_5=2, x_6=0\}
%     \end{align}
% \end{equation}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
